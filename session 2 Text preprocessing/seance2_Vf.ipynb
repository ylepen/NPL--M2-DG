{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ec9e8cb-d07e-4a8c-8a53-69c11864d69a",
   "metadata": {},
   "source": [
    "# Preparing textual data for statistics and machine learning\n",
    "\n",
    "The purpose of this session is to use Python specialized libraries to prepare a sample of text for a subsequent quantitative analysis, for instance text classification. \n",
    "The differents steps of the process are:\n",
    "\n",
    "1. Importing the dataset\n",
    "2. Cleaning the dataset\n",
    "3. Tokenization\n",
    "4. Feature extraction on a large dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362cb11-fc85-4587-85d5-5c5c48ed2a8e",
   "metadata": {},
   "source": [
    "##  Data\n",
    "\n",
    "We use data of the reddit self-post classification task on Kaggle (https://www.kaggle.com/datasets/mswarbrickjones/reddit-selfposts)\n",
    "\n",
    "Reddit (https://www.reddit.com/) is a social media website.  A subreddit is a specific online community, and the posts associated with it. \n",
    "\n",
    "Subreddits are dedicated to a particular topic that people write about, and they're denoted by /r/, followed by the subreddit's name, e.g., /r/gaming.\n",
    "\n",
    "We have two datasets:\n",
    "\n",
    "1. **rspct.tsv**\n",
    "\n",
    "This dataset consists of 1.013M self-posts, posted from 1013 subreddits (1000 examples per class). \n",
    "\n",
    "For each post we give:\n",
    "- the subreddit, \n",
    "- the title,\n",
    "- the content of the self-post.\n",
    "\n",
    "On this file, observations are separated by a tab\n",
    "\n",
    "\n",
    "2. **subreddit_info.csv**\n",
    "\n",
    "Contains manual annotation of about 3000 subreddits :\n",
    "\n",
    "- a top-level category and subcategory for each subreddit, \n",
    "\n",
    "- a reason for exclusion if this does not appear in the data.\n",
    "\n",
    "\n",
    "As a first step, we will:\n",
    "\n",
    "- Import these two datasets\n",
    "- Make a joint dataframe between these two dataframe based on the subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4506e871-57cb-4f0c-b679-f0e795b49950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c632ab22-6b22-4ccd-8ce4-58f784923010",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_file = \"rspct.tsv\"\n",
    "\n",
    "posts_df = pd.read_csv(posts_file, sep='\\t')\n",
    "\n",
    "posts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b2076-99dd-4e71-84da-9973474ab9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace1fc76-2c3e-4271-932f-1ff1fef40465",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55efb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of subreddit\n",
    "posts_df['subreddit'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79afb15-ec81-461c-a2f4-f3f13cc6f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=posts_df['subreddit']=='whatsthatbook'\n",
    "posts_df.loc[mask,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a8dad-f3ac-420f-a9c1-bef5b1ea48f9",
   "metadata": {},
   "source": [
    "**subreddit_info.csv**\n",
    "\n",
    "Contains manual annotation of about 3000 subreddits :\n",
    "    \n",
    "    - a top-level category and subcategory for each subreddit, \n",
    "    \n",
    "    - a reason for exclusion if this does not appear in the data.\n",
    "\n",
    "These information can be considerered as  **metadata**: information on characteristics of the text (and not the content of the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50604f50-6368-47fc-a259-a5dccc477120",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_file = \"subreddit_info.csv\"\n",
    "subred_df=pd.read_csv(subred_file)\n",
    "subred_df.info()\n",
    "subred_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efa161-55ea-4fb1-8083-56a5da03c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_file = \"subreddit_info.csv\"\n",
    "subred_df=pd.read_csv(subred_file).set_index(['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b480472-8b71-484a-a346-0950890dfb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3126d21-3b74-43bb-816b-ebe1f4889002",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abad72-5c92-4ad8-b5e8-86960c0eff4a",
   "metadata": {},
   "source": [
    "## Joining the two dataframes ##\n",
    "\n",
    "We want to gather the two previous datasets, on the basis of the subreddit which is a column of posts_df and the index of subred_df. \n",
    "\n",
    "subreddit : column in the caller (posts_df) to join on the index of subred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c329eeb7-a88f-4cca-b4a5-ac81d7594dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=posts_df.join(subred_df, on ='subreddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239bf600-5864-4b67-a052-841da3a069ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe74f78-11c3-4df9-9c5d-d90d73644b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d298d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fbb6f-13de-4013-9d81-6b7055ae1b1e",
   "metadata": {},
   "source": [
    "### Standardizing Attributes Names\n",
    "\n",
    "Usual practise:\n",
    "- **df**: name of the dataset\n",
    "- **text**: name of the column containing text to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7259734-9e5d-4a13-8489-bea048bf1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2c48d-a5bb-4a5f-a9f7-7005f22cda63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['category_3', 'in_data', 'reason_for_exclusion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05077c1-ba20-4d86-91ab-f9cb27b0379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    'selftext':'text',\n",
    "    'category_1':'category',\n",
    "    'category_2':'subcategory',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed921e5-792f-4689-b045-11b269435224",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns=column_mapping)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4390aca-456c-4f54-a569-d18f2cc4f111",
   "metadata": {},
   "source": [
    "#### Renaming columns and suppressing NaN columns - alternative method\n",
    "\n",
    "- selftext renamed as text\n",
    "- category_1 renamed as category\n",
    "- category_2 renamed as subcategory\n",
    "\n",
    "\n",
    " category_3, in_data and reason_for_exclusion **are suppressed (incomplete data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339721b-0800-454c-aab2-f4235f22c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    'id':'id',\n",
    "    'subreddit':'subreddit',\n",
    "    'title':'title',\n",
    "    'selftext':'text',\n",
    "    'category_1':'category',\n",
    "    'category_2':'subcategory',\n",
    "    'category_3': None,\n",
    "    'in_data': None,\n",
    "    'reason_for_exclusion': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4d0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941a533-a395-4e03-a071-9eb6dc84412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[c for c in column_mapping.keys() if column_mapping[c] != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f18a66-26c3-45f9-9a77-78f1a272bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baea0af-8b87-4f01-846d-7da9cf0e8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[columns].rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b677be-1db6-42e5-9d65-c24ab87b314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed08a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d00aa3-ffcd-4ebc-925f-77453945f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['category'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdcbafb-0e18-4729-b48d-81d85bf790a1",
   "metadata": {},
   "source": [
    "### Selection of data for the autos category\n",
    "\n",
    "We restrict the data to the autos category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910bbb4-128c-4f9d-8f35-dbe11ca5f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['category']=='autos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee265aa4-d6af-4ddb-879c-69fe174680d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61389955-b00a-457e-a9bd-f674f7a5b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadc135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49f77b-71e1-45b1-a1b0-57e245925ab7",
   "metadata": {},
   "source": [
    "## Python libraries\n",
    "\n",
    "Two associated Python libraries:\n",
    "\n",
    "**textacy**(https://pypi.org/project/textacy/)\n",
    "\n",
    "        preprocessing = clean, normalize and explore raw data before processing it with spaCy*\n",
    "        \n",
    "**spaCy** (https://spacy.io/)\n",
    "            \n",
    "        fundamentals = tokenization, part-of-speech tagging, dependency parsing..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d4c4ed-18fe-4b0f-a316-8c8af2492bd7",
   "metadata": {},
   "source": [
    "## Preliminary step: Cleaning Text Data with textacy\n",
    "\n",
    "We don't have well edited texts. There are several problems of quality that we need to take into account:\n",
    "\n",
    "- **Salutations, signatures and adresses**: usually not informative\n",
    "    \n",
    "\n",
    "- **Replies**: in case the text contains replies repeating the question, we need to eliminate the duplicated question. If not, we can introduce bias in the statistical analysis.\n",
    "    \n",
    "    \n",
    "- **Special formatting and program code**: in case, the text contain special characters, HTML entities, Mardown tags,...Necessary to eliminate these signs before the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d046998",
   "metadata": {},
   "source": [
    "- TextaCy module used to perform (preliminary/cleaning) NLP tasks on texts:\n",
    "    \n",
    "    - replacing and removing punctuation, extra whitespaces, numbers from the text before processing with spaCy\n",
    "    \n",
    "- Built upon the SpaCy module in Python\n",
    "\n",
    "https://www.geeksforgeeks.org/textacy-module-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3c70e-0c06-480f-809b-e9a320e1caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ccbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=df.loc[df.index[0],'text'] # selection of text by using df.index[list]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1733fa9a-78a2-4cfe-a4f7-e7630dc6737c",
   "metadata": {},
   "source": [
    "Raw text sometimes needs to be cleaned before analysis\n",
    "\n",
    "textacy.preprocessing sub-package contains a number of functions:\n",
    "\n",
    "- to normalize (whitespace, quotation marks,...)\n",
    "\n",
    "- remove (punctuations, accents,...)\n",
    "\n",
    "- replace (URLs, emails, numbers, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "import textacy.preprocessing as tprep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73372e0c-fb58-4dca-afba-906f724583ba",
   "metadata": {},
   "source": [
    "With make_pipeline, we make a callable pipeline which take a text as input, passes it through the functions in squential orders and then output a single preprocessed string text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d38dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = tprep.make_pipeline(\n",
    "    tprep.normalize.hyphenated_words,\n",
    "    tprep.normalize.quotation_marks,\n",
    "    tprep.normalize.unicode,\n",
    "    tprep.normalize.whitespace,\n",
    "    tprep.remove.html_tags,\n",
    "    tprep.remove.accents,\n",
    "    tprep.remove.punctuation,\n",
    "    tprep.remove.brackets,\n",
    "    tprep.replace.numbers,\n",
    "    tprep.replace.urls,\n",
    "    tprep.replace.currency_symbols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d63aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=preproc(text)\n",
    "\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a10525c-dc07-452e-ba8c-608f088b75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2= 'There is (no) of these 10 examples of 100 £ loans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addfdd4e-f51f-4d92-b197-4c4df19efcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c80ab3",
   "metadata": {},
   "source": [
    "### Alternative: creating a specific function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09957002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = tprep.replace.urls(text)# we replace url with text\n",
    "    text = tprep.remove.html_tags(text)\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    text = tprep.remove.punctuation(text)\n",
    "    text = tprep.normalize.whitespace(text)\n",
    "    text = tprep.replace.numbers(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c06cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51952682-c3e1-48ff-810b-8a519dc100c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df.loc[df.index[:5],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db730e2-dd46-4437-8872-7f372ee9e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592e49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small['text'].apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0fb518-de58-41d2-93e0-14453b224287",
   "metadata": {},
   "source": [
    "## Linguistic Processing with spaCy\n",
    "\n",
    "- Spacy: library for linguistic data processing\n",
    "\n",
    "- spaCy's pipeline is language dependent: we hav to load a particular pipeline to process the text \n",
    "    \n",
    "- Spacy provide an integrated pipeline of processing documents:\n",
    "    \n",
    "    1. a tokenizer (by default) : tok2vec\n",
    "    2. a part-of-speech tagger : tagger\n",
    "    3. a dependency parser : parser\n",
    "    4. a sentence recognizer : senter\n",
    "    5. a attribute ruler \n",
    "    6. a lemmatizer : lemmatizer\n",
    "    7. a named-entity recognizer : ner\n",
    "    \n",
    "- the tokenizes is based on language-dependent rules = > fast\n",
    "\n",
    "\n",
    "- 2, 3 and 4 are based on pretrained neural models => can 10-20 times as long as tokenization\n",
    "\n",
    "- The initial input is a text\n",
    "\n",
    "- The final output is a **Doc** object\n",
    "\n",
    "- The **Doc** object contains a list of **Tokens** objects\n",
    "\n",
    "- Any range selection of tokens creates a **Span**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4eea1c-b1b4-4c14-b35d-f26a02ec08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "We import spaCy one of trained pipelines for english \n",
    "\n",
    "For example, en_core_web_sm is a small English pipeline trained on was trained on an annotated corpus called “OntoNotes”: 2 million+ words drawn from “news, broadcast, talk shows, weblogs, usenet newsgroups, and conversational telephone speech,” which were meticulously tagged by a group of researchers and professionals for people’s names and places, for nouns and verbs, for subjects and objects, and much more.\n",
    "\n",
    "https://spacy.io/models/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6c1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac4328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'en_core_wb_sm' is the name of the installed spaCy pipeline\n",
    "from spacy.cli import download\n",
    "print(download('en_core_web_sm'))\n",
    "#print(download('en_core_web_md'))\n",
    "#print(download('en_core_web_lg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ddc60c-5130-48c2-a1d5-0b9e1eb0dd36",
   "metadata": {},
   "source": [
    "We make a spaCy **Doc** from text\n",
    "\n",
    "A doc is required as inputs of the functions of spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8206d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = textacy.make_spacy_doc(clean_text,lang=\"en_core_web_sm\")\n",
    "doc._.preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d6db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e098bb-f2ae-4123-8278-b96684f1fb22",
   "metadata": {},
   "source": [
    "### Alternative code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b596dbb-c185-477d-9199-e7fdc90763a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12670d-3ecf-4a6e-8ab9-5541724a58a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d335588-7ea8-4e18-90a2-dd6697d4f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_alt = nlp(clean_text)\n",
    "print(doc_alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af1906",
   "metadata": {},
   "source": [
    "### Displaying tokens in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28683a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0abf3-1f98-4b29-9c7b-7c3c1023cd56",
   "metadata": {},
   "source": [
    "### Tokens have attributes \n",
    "\n",
    "    - token.is_punct  : Is the token punctuation? \n",
    "    - token.is_alpha  : Does the token consist of alphabetic characters? \n",
    "    - token.like_email : Does the token resemble an email address?\n",
    "    - token.like_url : : Does the token resemble a URL?\n",
    "\n",
    "    - token.is_stop : Is the token part of a “stop list”?\n",
    "    - token.lemma_ : Base form of the token, with no inflectional suffixes.\n",
    "    - token.pos : core part-of-speech categories https://universaldependencies.org/u/pos/\n",
    "            \n",
    "            \n",
    "See https://spacy.io/api/token for the list of all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc04fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token,token.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying alphabetical characters\n",
    "for token in doc:\n",
    "    print(token,token.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying stop words in a document\n",
    "for token in doc:\n",
    "    print(token,token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc47ec-0b3d-4b9c-b1f0-6182f4373455",
   "metadata": {},
   "source": [
    "## Tag-of-speech\n",
    "\n",
    "- **part-of-speech** are the grammatical units of language: verbs, nouns, adjectives, adverbs, pronouns, prepositions\n",
    "\n",
    "- part-of-speech can be used to explore syntax\n",
    "\n",
    "- - Each token in a spaCy doc has two part-of-speech attributes:\n",
    "    - pos_\n",
    "    - tag_\n",
    "- tag_ can be language specific \n",
    "- pos_ contains the simplified tag of the universal part-of-speech tagset\n",
    " \n",
    "- pos_ can be used as an alternative to stop words\n",
    "\n",
    "- pos_ can be classified into two categories \n",
    "\n",
    "- pronouns, prepositions, conjunctions, determiners: \n",
    "    - called **function words**\n",
    "    - their main function is to create grammatical relationships in a sentence\n",
    "    - not very informative\n",
    "\n",
    "- nouns, verbs, adjectives and adverbs: \n",
    "    - **content** words\n",
    "    - the meaning of a sentence depends on them\n",
    "    \n",
    "\n",
    "- We can use **part-of-speech tags** to select the word types\n",
    "\n",
    "https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/13-POS-Keywords.html#\n",
    "\n",
    "- Part-of-speech tags can be used to make a selection among tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264204b-cbd6-4b92-8c34-fb6e9dc2590f",
   "metadata": {},
   "source": [
    "spaCy has been trained to recognize pos_ according to the context in which the word appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92654237-c8e2-48ff-a1f7-2e9f18eab4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'You need to write an abstract'\n",
    "token_sentence1 = nlp(sentence1)\n",
    "for token in token_sentence1:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949147e6-59b4-4f6a-ac2d-80cdfba88b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = 'At his age, he still fails to abstract certain concepts'\n",
    "token_sentence2 = nlp(sentence2)\n",
    "for token in token_sentence2:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead88217-cc57-425c-ad24-f04b82a5954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence3 = \"He ages well\"\n",
    "token_sentence3 = nlp(sentence3)\n",
    "for token in token_sentence3:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f70c13-ecfd-449a-bf92-99afcc87f91a",
   "metadata": {},
   "source": [
    "### Tokens and pos_ of doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eceb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_, spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9299ec-4d8e-4788-a67b-c1da7d7799ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "We want to make the list of the nouns in doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83edd9f-bd55-439d-9480-2369f2078d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns=[]\n",
    "for token in doc:\n",
    "    if token.pos_== 'NOUN':\n",
    "       nouns.append(token.text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956211b-0c7e-4a4b-a855-e3e0ab4bfb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde4137e-fd91-4526-ba96-c52954f4e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "nouns_count = Counter(nouns)\n",
    "print(nouns_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5273582-1357-4374-ad9a-d93fdf23b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_count.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19ae7e-6efa-4c84-828d-d66ec74c0054",
   "metadata": {},
   "source": [
    "### Specific functions of Textacy to extract words according to their pos\n",
    "The output is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c0b41-b88f-4720-b041-ac87bc19d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_alt =textacy.extract.words(doc)\n",
    "print(list(token_alt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a618cd-41d6-4051-8e08-4d44a709b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input file must be a doc \n",
    "tokens1=textacy.extract.words(doc, include_pos={\"ADJ\",\"NOUN\"})\n",
    "print(list(tokens1))\n",
    "#print(*[t for t in tokens1], sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f9743-d454-4479-96b7-32b994ec99e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2=textacy.extract.words(doc, include_pos={\"ADJ\",\"NOUN\"},min_freq=2)\n",
    "print(list(tokens2)\n",
    "#print(*[t for t in tokens2], sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47fcb8-cc20-43c6-8a5e-ccdfaab7ef4f",
   "metadata": {},
   "source": [
    "### Tags \n",
    "A more detailled classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98548084-eaa7-4933-a294-760e6214b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token,token.tag_,spacy.explain(token.tag_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e985845c-2b34-4073-9b29-a75cd707807b",
   "metadata": {},
   "source": [
    "### dep_ structure of dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c38f0b-b6ff-4391-a868-5e2393fc59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499541fe-635f-46f4-9260-86d5ec837ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set some display options for the visualizer\n",
    "options = {\"compact\": True, \"distance\": 90, \"color\": \"yellow\", \"bg\": \"black\", \"font\": \"Gill Sans\"}\n",
    "\n",
    "displacy.render(token_sentence1, style=\"dep\", options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d386300-140d-4603-b699-c30880f8f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token,token.dep_,spacy.explain(token.dep_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d20d72",
   "metadata": {},
   "source": [
    "## Lemmatization/ Stemming\n",
    "\n",
    "- Replacing words with their root: \n",
    "    - \"economic\", \"economics\", \"economically\" all replaced by the stem (the root) \"economy\"\n",
    "    - Porter stemmer (Porter 1980): standard stemming tool for English language text\n",
    "- smaller vocabulary: increase speed of execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token,token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee8a17-b8d2-4e4b-8e70-d6fd6b981d10",
   "metadata": {},
   "source": [
    "### Analysis of a Doc\n",
    "\n",
    "- Extracting n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c99649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy import extract\n",
    "list(extract.ngrams(doc,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c0f82",
   "metadata": {},
   "source": [
    "### Remark: We can discard some function of the spaCy pipeline\n",
    "\n",
    "We can import selected elements of the pipeline if some component are useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4021ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_2=spacy.load('en_core_web_sm', disable=[\"parser\",\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345258cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_2.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43245c01",
   "metadata": {},
   "source": [
    "## Working with stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21685c1",
   "metadata": {},
   "source": [
    "- spaCy uses language-specific stop word lists to set the is_stop property for each token\n",
    "- Filtering stop words (and punctuation tokens) is easy\n",
    "- The list of stop words is loaded when a nlp object is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af471fbe",
   "metadata": {},
   "source": [
    "### The list of stop words can be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67add01",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['down'].is_stop=False\n",
    "nlp.vocab['Dear'].is_stop=True\n",
    "nlp.vocab['Regards'].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2374ceb7",
   "metadata": {},
   "source": [
    "### Extracting Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lemmas(doc,**kwargs):\n",
    "    return[t.lemma_ for t in textacy.extract.words(doc,**kwargs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc080aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_doc = extract_lemmas(doc,min_freq=2)\n",
    "print(*tokenized_doc, sep = \"|\")\n",
    "len(tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_doc = extract_lemmas(doc,  include_pos={\"ADJ\",\"NOUN\"})\n",
    "print(*tokenized_doc, sep = \"|\")\n",
    "len(tokenized_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de1950",
   "metadata": {},
   "source": [
    "### Extracting Named entities\n",
    "\n",
    "- The process of detecting entities such as people, locations, organization in texts\n",
    "- In the **Named-entity recognizer** attributes of Doc:\n",
    "    - Doc.ents\n",
    "    - Token.ent_iob_\n",
    "    - Token.ent_type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text0=df.loc[df.index[0],'text'] # selection of text by using df.index[list]\n",
    "print(text0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df76c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesssing with textacy pipeline\n",
    "clean_text0=preproc(text0)\n",
    "\n",
    "print(clean_text0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bcb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc0 = textacy.make_spacy_doc(clean_text0,lang=\"en_core_web_sm\")\n",
    "doc0._.preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(textacy.extract.entities(doc, include_types={\"DATE\",\"PRODUCT\",\"ORG\",\"LOCATION\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49882074",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f\"({ent.text},{ent.label_})\",end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdf304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc,style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1816c13",
   "metadata": {},
   "source": [
    "# Make a Corpus\n",
    "\n",
    "A textacy.Corpus is an ordered collection of spaCy Doc all processed by the same language pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1798330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "records=df['text']\n",
    "\n",
    "preproc_records=((preproc(text)) for text in records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=textacy.Corpus(\"en_core_web_sm\",data=preproc_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b40d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.n_docs, corpus.n_sents, corpus.n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e71825",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0]._.preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd20a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0eff75-c266-4260-b320-9497584e4424",
   "metadata": {},
   "source": [
    "### Transforming a corpus into an array \n",
    "\n",
    "**textacy.representations.vectorizers** : Transform a collection of tokenized docs into a **doc-term matrix** of shape (# docs, # unique terms), with various ways to filter or limit included terms and flexible weighting schemes for their values.\n",
    "    \n",
    "    \n",
    "https://textacy.readthedocs.io/en/latest/api_reference/representations.html#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = ((term.lemma_ for term in textacy.extract.words(doc,include_pos={\"ADJ\",\"NOUN\"})) for doc in corpus[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.representations import Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e6f87-5691-4dd9-9360-896370b937ef",
   "metadata": {},
   "source": [
    "### Specification of the Vectorizer\n",
    "tf_type : specify the type of type frequency\n",
    "    tf_type = linear \n",
    "\n",
    "tf_type = can be linear, sqrt, log, binary\n",
    "\n",
    "idf_type : Type of inverse document frequency (idf) to use for weights’ global \n",
    "        can be standard, smooth,bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_alt = Vectorizer( tf_type=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da857d-4293-4961-b2a2-6d8a159678c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_alt.weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac142f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix_alt = vectorizer_alt.fit_transform(tokenized_docs)\n",
    "doc_term_matrix_alt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56012729-5a01-45ae-aede-8b54f4b34db6",
   "metadata": {},
   "source": [
    "Terms associated with columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada786f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_alt.terms_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_term_matrix_alt[:20, vectorizer_alt.vocabulary_terms[\"story\"]].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b916cc-2605-4baf-b335-32ffe00e490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs_n = ((term.lemma_ for term in textacy.extract.words(doc,include_pos={\"ADJ\",\"NOUN\"})) for doc in corpus[21:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa1554b-d33b-4485-aad3-2d4017385a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_matrix_terms_alt_n = vectorizer_alt.transform(tokenized_docs_n)\n",
    "doc_matrix_terms_alt_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfda4d0-8f08-4df0-a314-86a2175d99bc",
   "metadata": {},
   "source": [
    "## Another example of tokenization and vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a21c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = ((term.lemma_ for term in textacy.extract.words(doc,include_pos={\"VERB\"})) for doc in corpus[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba15709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = Vectorizer( tf_type=\"linear\")\n",
    "vectorizer = Vectorizer(tf_type=\"linear\", idf_type=\"standard\",min_df=5, max_df=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b1ab80-d4ff-46d9-8a69-dbda3b1c7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = vectorizer.fit_transform(tokenized_docs)\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_term_matrix[:20, vectorizer.vocabulary_terms[\"know\"]].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cde6a-5dce-46d4-9e83-83502c56fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = ((term.lemma_ for term in textacy.extract.words(doc,include_pos={\"VERB\"})) for doc in corpus[21:41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9723958-9b7d-4e19-933c-f6f1ac8790f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_matrix_terms= vectorizer.transform(tokenized_docs)\n",
    "doc_matrix_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a69aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_matrix_terms[:20, vectorizer.vocabulary_terms[\"know\"]].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe90833b-83b2-4ddb-bfe4-e430120dd30a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
