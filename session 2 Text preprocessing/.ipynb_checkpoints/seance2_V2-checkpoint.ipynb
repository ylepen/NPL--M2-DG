{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8037af-4fdb-4dc6-bab5-c3c7fe40c4da",
   "metadata": {},
   "source": [
    "# Preparing textual data for statistics and machine learning\n",
    "\n",
    "1. Importing the dataset\n",
    "2. Cleaning the dataset\n",
    "3. Tokenization\n",
    "4. Feature extraction on a large dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c65d6e",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "\n",
    "Reddit (https://www.reddit.com/) self-Posts dataset avalaible on Kaggle\n",
    "\n",
    "The data consists of 1.013M self-posts, posted from 1013 subreddits (1000 examples per class). For each post we give the subreddit, the title and content of the self-post.\n",
    "\n",
    "A subreddit is a specific online community, and the posts associated with it, on the social media website Reddit. Subreddits are dedicated to a particular topic that people write about, and they're denoted by /r/, followed by the subreddit's name, e.g., /r/gaming.\n",
    "\n",
    "For each post we give the subreddit, the title and content of the self-post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdee6a3-b909-4dae-b994-af8e159e2444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c632ab22-6b22-4ccd-8ce4-58f784923010",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_file = \"rspct.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "954dd061-a436-4de6-ba2c-cbcb19e7805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.read_csv(posts_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49a80ce9-5c85-42e0-b4b4-51e269561517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1013000 entries, 0 to 1012999\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   id         1013000 non-null  object\n",
      " 1   subreddit  1013000 non-null  object\n",
      " 2   title      1013000 non-null  object\n",
      " 3   selftext   1013000 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 30.9+ MB\n"
     ]
    }
   ],
   "source": [
    "posts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ace1fc76-2c3e-4271-932f-1ff1fef40465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6d8knd</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58mbft</td>\n",
       "      <td>teenmom</td>\n",
       "      <td>So what was Matt \"addicted\" to?</td>\n",
       "      <td>Did he ever say what his addiction was or is h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6ti6re</td>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>Not door bell, but floodlight mount height.</td>\n",
       "      <td>I know this is a sub for the 'Ring Doorbell' b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77sxto</td>\n",
       "      <td>intel</td>\n",
       "      <td>Worried about my 8700k small fft/data stress r...</td>\n",
       "      <td>Prime95 (regardless of version) and OCCT both,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             subreddit  \\\n",
       "0  6d8knd  talesfromtechsupport   \n",
       "1  58mbft               teenmom   \n",
       "2  8f73s7                Harley   \n",
       "3  6ti6re          ringdoorbell   \n",
       "4  77sxto                 intel   \n",
       "\n",
       "                                               title  \\\n",
       "0             Remember your command line switches...   \n",
       "1                    So what was Matt \"addicted\" to?   \n",
       "2                                     No Club Colors   \n",
       "3        Not door bell, but floodlight mount height.   \n",
       "4  Worried about my 8700k small fft/data stress r...   \n",
       "\n",
       "                                            selftext  \n",
       "0  Hi there,  <lb>The usual. Long time lerker, fi...  \n",
       "1  Did he ever say what his addiction was or is h...  \n",
       "2  Funny story. I went to college in Las Vegas. T...  \n",
       "3  I know this is a sub for the 'Ring Doorbell' b...  \n",
       "4  Prime95 (regardless of version) and OCCT both,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55efb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1013"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## number of subredddit\n",
    "posts_df['subreddit'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cba941",
   "metadata": {},
   "source": [
    "**subreddit_info.csv**\n",
    "\n",
    "Contains manual annotation of about 3000 subreddits :\n",
    "    \n",
    "    - a top-level category and subcategory for each subreddit, \n",
    "    \n",
    "    - a reason for exclusion if this does not appear in the data.\n",
    "\n",
    "These information can be considerered as  **metadata**: information on characteristics of the text (and not the content of the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8efa161-55ea-4fb1-8083-56a5da03c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_file = \"subreddit_info.csv\"\n",
    "subred_df=pd.read_csv(subred_file).set_index(['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b480472-8b71-484a-a346-0950890dfb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3126d21-3b74-43bb-816b-ebe1f4889002",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15866ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_df['in_data'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3fdcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=(subred_df['in_data']==True).value_counts()\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc4357-9190-41e1-8296-2af93c319ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_df.loc['Harley']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c329eeb7-a88f-4cca-b4a5-ac81d7594dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=posts_df.join(subred_df, on ='subreddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239bf600-5864-4b67-a052-841da3a069ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe74f78-11c3-4df9-9c5d-d90d73644b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d298d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fbb6f-13de-4013-9d81-6b7055ae1b1e",
   "metadata": {},
   "source": [
    "### Standardizing Attributes Names\n",
    "\n",
    "Usual practise:\n",
    "- **df**: name of the dataset\n",
    "- **text**: name of the column containing text to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7259734-9e5d-4a13-8489-bea048bf1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac938820",
   "metadata": {},
   "source": [
    "#### Renaming columns\n",
    "\n",
    "- selftext renamed as text\n",
    "- category_1 renamed as category\n",
    "- category_2 renamed as subcategory\n",
    "\n",
    "\n",
    " category_3, in_data and reason_for_exclusion **are suppressed (incomplete data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7339721b-0800-454c-aab2-f4235f22c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    'id':'id',\n",
    "    'subreddit':'subreddit',\n",
    "    'title':'title',\n",
    "    'selftext':'text',\n",
    "    'category_1':'category',\n",
    "    'category_2':'subcategory',\n",
    "    'category_3': None,\n",
    "    'in_data': None,\n",
    "    'reason_for_exclusion': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a941a533-a395-4e03-a071-9eb6dc84412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[c for c in column_mapping.keys() if column_mapping[c] != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43f18a66-26c3-45f9-9a77-78f1a272bd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'subreddit', 'title', 'selftext', 'category_1', 'category_2']\n"
     ]
    }
   ],
   "source": [
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8baea0af-8b87-4f01-846d-7da9cf0e8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[columns].rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47b677be-1db6-42e5-9d65-c24ab87b314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'subreddit', 'title', 'text', 'category', 'subcategory'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78712870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6d8knd</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, fi...</td>\n",
       "      <td>writing/stories</td>\n",
       "      <td>tech support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58mbft</td>\n",
       "      <td>teenmom</td>\n",
       "      <td>So what was Matt \"addicted\" to?</td>\n",
       "      <td>Did he ever say what his addiction was or is h...</td>\n",
       "      <td>tv_show</td>\n",
       "      <td>teen mom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "      <td>autos</td>\n",
       "      <td>harley davidson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6ti6re</td>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>Not door bell, but floodlight mount height.</td>\n",
       "      <td>I know this is a sub for the 'Ring Doorbell' b...</td>\n",
       "      <td>hardware/tools</td>\n",
       "      <td>doorbells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77sxto</td>\n",
       "      <td>intel</td>\n",
       "      <td>Worried about my 8700k small fft/data stress r...</td>\n",
       "      <td>Prime95 (regardless of version) and OCCT both,...</td>\n",
       "      <td>electronics</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             subreddit  \\\n",
       "0  6d8knd  talesfromtechsupport   \n",
       "1  58mbft               teenmom   \n",
       "2  8f73s7                Harley   \n",
       "3  6ti6re          ringdoorbell   \n",
       "4  77sxto                 intel   \n",
       "\n",
       "                                               title  \\\n",
       "0             Remember your command line switches...   \n",
       "1                    So what was Matt \"addicted\" to?   \n",
       "2                                     No Club Colors   \n",
       "3        Not door bell, but floodlight mount height.   \n",
       "4  Worried about my 8700k small fft/data stress r...   \n",
       "\n",
       "                                                text         category  \\\n",
       "0  Hi there,  <lb>The usual. Long time lerker, fi...  writing/stories   \n",
       "1  Did he ever say what his addiction was or is h...          tv_show   \n",
       "2  Funny story. I went to college in Las Vegas. T...            autos   \n",
       "3  I know this is a sub for the 'Ring Doorbell' b...   hardware/tools   \n",
       "4  Prime95 (regardless of version) and OCCT both,...      electronics   \n",
       "\n",
       "       subcategory  \n",
       "0     tech support  \n",
       "1         teen mom  \n",
       "2  harley davidson  \n",
       "3        doorbells  \n",
       "4              cpu  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba5d7d",
   "metadata": {},
   "source": [
    "### Selection of data for the autos category\n",
    "\n",
    "We restrict the data to the auto category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1910bbb4-128c-4f9d-8f35-dbe11ca5f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['category']=='autos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee265aa4-d6af-4ddb-879c-69fe174680d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61389955-b00a-457e-a9bd-f674f7a5b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bbcab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f47e27",
   "metadata": {},
   "source": [
    "## Python libraries\n",
    "\n",
    "Two associated Python libraries:\n",
    "\n",
    "**textacy**\n",
    "    \n",
    "        preprocessing = clean, normalize and explore raw data before processing it with spaCy*\n",
    "        \n",
    "**spaCy** : \n",
    "        \n",
    "        fundamentals = tokenization, part-of-speech tagging, dependency parsing..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30403b51",
   "metadata": {},
   "source": [
    "## Preliminary step: Cleaning Text Data with textacy\n",
    "\n",
    "We don't have well edited texts. There are several problems of quality that we need to take into account:\n",
    "\n",
    "- **Salutations, signatures and adresses**: usually not informative\n",
    "    \n",
    "\n",
    "- **Replies**: in case the text contains replies repeating the question, we need to eliminate the duplicated question. If not, we can introduce bias in the statistical analysis.\n",
    "    \n",
    "    \n",
    "- **Special formatting and program code**: in case, the text contain special characters, HTML entities, Mardown tags,...Necessary to eliminate these signs before the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe2433",
   "metadata": {},
   "source": [
    "- TextaCy module used to perform (preliminary/cleaning) NLP tasks on texts:\n",
    "    \n",
    "    - replacing and removing punctuation, extra whitespaces, numbers from the text before processing with spaCy\n",
    "    \n",
    "- Built upon the SpaCy module in Python\n",
    "\n",
    "https://www.geeksforgeeks.org/textacy-module-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "477ccbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.cars.com/articles/how-often-should-i-change-engine-coolant-1420680853669/<lb><lb>I have a IS 250 AWD from 2006. About 73K miles on it. I've never touched the engine radiator coolant and can't find anything on when to change this in the book. It just says 'long life 100k Toyota coolant.' <lb><lb>Does anyone get this flushed or changed at ten years?? Do I wait until 100k? \n"
     ]
    }
   ],
   "source": [
    "text=df.loc[df.index[3],'text'] # selection of text by using df.index[list]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "118b6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "import textacy.preprocessing as tprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00d38dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = tprep.make_pipeline(\n",
    "    tprep.replace.urls,\n",
    "    tprep.remove.html_tags,\n",
    "    tprep.normalize.hyphenated_words,\n",
    "    tprep.normalize.quotation_marks,\n",
    "    tprep.normalize.unicode,\n",
    "    tprep.remove.accents,\n",
    "    tprep.remove.punctuation,\n",
    "    tprep.normalize.whitespace,\n",
    "    tprep.replace.numbers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47d63aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL have a IS _NUMBER_ AWD from _NUMBER_ About 73K miles on it I ve never touched the engine radiator coolant and can t find anything on when to change this in the book It just says long life 100k Toyota coolant Does anyone get this flushed or changed at ten years Do I wait until 100k\n"
     ]
    }
   ],
   "source": [
    "clean_text=preproc(text)\n",
    "\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f294588",
   "metadata": {},
   "source": [
    "### Alternative: creating a specific function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09957002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = tprep.replace.urls(text)# we replace url with text\n",
    "    text = tprep.remove.html_tags(text)\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    text = tprep.remove.punctuation(text)\n",
    "    text = tprep.normalize.whitespace(text)\n",
    "    text = tprep.replace.numbers(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c06cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalize(text))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "522e01fb",
   "metadata": {},
   "source": [
    "## Linguistic Processing with spaCy\n",
    "\n",
    "- Spacy: library for linguistic data processing\n",
    "    \n",
    "- Spacy provide an integrated pipeline of processing documents:\n",
    "    \n",
    "    1. a tokenizer (by default)\n",
    "    2. a part-of-speech tagger  \n",
    "    3. a dependency parser\n",
    "    4. a named-entity recognizer\n",
    "    5. a lemmatizer\n",
    "    \n",
    "- the tokenizes is based on language-dependent rules = > fast\n",
    "\n",
    "\n",
    "- 2, 3 and 4 are based on pretrained neural models => can 10-20 times as long as tokenization\n",
    "\n",
    "- The initial input is a text\n",
    "\n",
    "- The final output is a **Doc** object\n",
    "\n",
    "- The **Doc** object contains a list of **Tokens** objects\n",
    "\n",
    "- Any range selection of tokens creates a **Span**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cbbae",
   "metadata": {},
   "source": [
    "We import spaCy one of trained pipelines for english \n",
    "\n",
    "https://spacy.io/models/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca6c1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ac4328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from spacy.cli import download\n",
    "print(download('en_core_web_sm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea8206d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(62 tokens: \"URL have a IS _NUMBER_ AWD from _NUMBER_ About ...\")'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = textacy.make_spacy_doc(clean_text,lang=\"en_core_web_sm\")\n",
    "doc._.preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6e25760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL have a IS _NUMBER_ AWD from _NUMBER_ About 73K miles on it I ve never touched the engine radiator coolant and can t find anything on when to change this in the book It just says long life 100k Toyota coolant Does anyone get this flushed or changed at ten years Do I wait until 100k\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8da477",
   "metadata": {},
   "source": [
    "### Displaying tokens in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7c9c224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL\n",
      "have\n",
      "a\n",
      "IS\n",
      "_\n",
      "NUMBER\n",
      "_\n",
      "AWD\n",
      "from\n",
      "_\n",
      "NUMBER\n",
      "_\n",
      "About\n",
      "73\n",
      "K\n",
      "miles\n",
      "on\n",
      "it\n",
      "I\n",
      "ve\n",
      "never\n",
      "touched\n",
      "the\n",
      "engine\n",
      "radiator\n",
      "coolant\n",
      "and\n",
      "can\n",
      "t\n",
      "find\n",
      "anything\n",
      "on\n",
      "when\n",
      "to\n",
      "change\n",
      "this\n",
      "in\n",
      "the\n",
      "book\n",
      "It\n",
      "just\n",
      "says\n",
      "long\n",
      "life\n",
      "100k\n",
      "Toyota\n",
      "coolant\n",
      "Does\n",
      "anyone\n",
      "get\n",
      "this\n",
      "flushed\n",
      "or\n",
      "changed\n",
      "at\n",
      "ten\n",
      "years\n",
      "Do\n",
      "I\n",
      "wait\n",
      "until\n",
      "100k\n"
     ]
    }
   ],
   "source": [
    "for tok in doc:\n",
    "    print(tok.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55532047",
   "metadata": {},
   "source": [
    "### Tokens have attributes \n",
    "\n",
    "    - token.is_punct  : Is the token punctuation? \n",
    "    - token.is_alpha  : Does the token consist of alphabetic characters? \n",
    "    - token.like_email : Does the token resemble an email address?\n",
    "    - token.like_url : : Does the token resemble a URL?\n",
    "    - token.is_stop : Is the token part of a “stop list”?\n",
    "    - token.lemma_ : Base form of the token, with no inflectional suffixes.\n",
    "    - token.pos : core part-of-speech categories https://universaldependencies.org/u/pos/\n",
    "            \n",
    "            \n",
    "See https://spacy.io/api/token for the list of all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcc04fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL False\n",
      "have False\n",
      "a False\n",
      "IS False\n",
      "_ True\n",
      "NUMBER False\n",
      "_ True\n",
      "AWD False\n",
      "from False\n",
      "_ True\n",
      "NUMBER False\n",
      "_ True\n",
      "About False\n",
      "73 False\n",
      "K False\n",
      "miles False\n",
      "on False\n",
      "it False\n",
      "I False\n",
      "ve False\n",
      "never False\n",
      "touched False\n",
      "the False\n",
      "engine False\n",
      "radiator False\n",
      "coolant False\n",
      "and False\n",
      "can False\n",
      "t False\n",
      "find False\n",
      "anything False\n",
      "on False\n",
      "when False\n",
      "to False\n",
      "change False\n",
      "this False\n",
      "in False\n",
      "the False\n",
      "book False\n",
      "It False\n",
      "just False\n",
      "says False\n",
      "long False\n",
      "life False\n",
      "100k False\n",
      "Toyota False\n",
      "coolant False\n",
      "Does False\n",
      "anyone False\n",
      "get False\n",
      "this False\n",
      "flushed False\n",
      "or False\n",
      "changed False\n",
      "at False\n",
      "ten False\n",
      "years False\n",
      "Do False\n",
      "I False\n",
      "wait False\n",
      "until False\n",
      "100k False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token,token.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27fd334f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL True\n",
      "have True\n",
      "a True\n",
      "IS True\n",
      "_ False\n",
      "NUMBER True\n",
      "_ False\n",
      "AWD True\n",
      "from True\n",
      "_ False\n",
      "NUMBER True\n",
      "_ False\n",
      "About True\n",
      "73 False\n",
      "K True\n",
      "miles True\n",
      "on True\n",
      "it True\n",
      "I True\n",
      "ve True\n",
      "never True\n",
      "touched True\n",
      "the True\n",
      "engine True\n",
      "radiator True\n",
      "coolant True\n",
      "and True\n",
      "can True\n",
      "t True\n",
      "find True\n",
      "anything True\n",
      "on True\n",
      "when True\n",
      "to True\n",
      "change True\n",
      "this True\n",
      "in True\n",
      "the True\n",
      "book True\n",
      "It True\n",
      "just True\n",
      "says True\n",
      "long True\n",
      "life True\n",
      "100k False\n",
      "Toyota True\n",
      "coolant True\n",
      "Does True\n",
      "anyone True\n",
      "get True\n",
      "this True\n",
      "flushed True\n",
      "or True\n",
      "changed True\n",
      "at True\n",
      "ten True\n",
      "years True\n",
      "Do True\n",
      "I True\n",
      "wait True\n",
      "until True\n",
      "100k False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token,token.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7561177d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL False\n",
      "have True\n",
      "a True\n",
      "IS True\n",
      "_ False\n",
      "NUMBER False\n",
      "_ False\n",
      "AWD False\n",
      "from True\n",
      "_ False\n",
      "NUMBER False\n",
      "_ False\n",
      "About True\n",
      "73 False\n",
      "K False\n",
      "miles False\n",
      "on True\n",
      "it True\n",
      "I True\n",
      "ve False\n",
      "never True\n",
      "touched False\n",
      "the True\n",
      "engine False\n",
      "radiator False\n",
      "coolant False\n",
      "and True\n",
      "can True\n",
      "t False\n",
      "find False\n",
      "anything True\n",
      "on True\n",
      "when True\n",
      "to True\n",
      "change False\n",
      "this True\n",
      "in True\n",
      "the True\n",
      "book False\n",
      "It True\n",
      "just True\n",
      "says False\n",
      "long False\n",
      "life False\n",
      "100k False\n",
      "Toyota False\n",
      "coolant False\n",
      "Does True\n",
      "anyone True\n",
      "get True\n",
      "this True\n",
      "flushed False\n",
      "or True\n",
      "changed False\n",
      "at True\n",
      "ten True\n",
      "years False\n",
      "Do True\n",
      "I True\n",
      "wait False\n",
      "until True\n",
      "100k False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token,token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0cc28",
   "metadata": {},
   "source": [
    "### Tag-of-speech\n",
    "\n",
    "- Refers to types of words are called **part-of-speech tags**\n",
    "\n",
    "- examples: nouns, verbs, adjectives\n",
    "\n",
    "- often important to restrict the types of words used to certain categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74a819fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL NOUN\n",
      "have VERB\n",
      "a DET\n",
      "IS PROPN\n",
      "_ PRON\n",
      "NUMBER NOUN\n",
      "_ PUNCT\n",
      "AWD PROPN\n",
      "from ADP\n",
      "_ PROPN\n",
      "NUMBER NOUN\n",
      "_ NOUN\n",
      "About ADV\n",
      "73 NUM\n",
      "K NOUN\n",
      "miles NOUN\n",
      "on ADP\n",
      "it PRON\n",
      "I PRON\n",
      "ve AUX\n",
      "never ADV\n",
      "touched VERB\n",
      "the DET\n",
      "engine NOUN\n",
      "radiator NOUN\n",
      "coolant NOUN\n",
      "and CCONJ\n",
      "can AUX\n",
      "t NOUN\n",
      "find VERB\n",
      "anything PRON\n",
      "on ADP\n",
      "when SCONJ\n",
      "to PART\n",
      "change VERB\n",
      "this PRON\n",
      "in ADP\n",
      "the DET\n",
      "book NOUN\n",
      "It PRON\n",
      "just ADV\n",
      "says VERB\n",
      "long ADJ\n",
      "life NOUN\n",
      "100k NUM\n",
      "Toyota PROPN\n",
      "coolant NOUN\n",
      "Does AUX\n",
      "anyone PRON\n",
      "get VERB\n",
      "this PRON\n",
      "flushed ADJ\n",
      "or CCONJ\n",
      "changed VERB\n",
      "at ADP\n",
      "ten NUM\n",
      "years NOUN\n",
      "Do AUX\n",
      "I PRON\n",
      "wait VERB\n",
      "until ADP\n",
      "100k NUM\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a26afa",
   "metadata": {},
   "source": [
    "## Lemmatization/ Stemming\n",
    "\n",
    "- Replacing words with their root: \n",
    "    - \"economic\", \"economics\", \"economically\" all replaced by the stem (the root) \"economy\"\n",
    "    - Porter stemmer (Porter 1980): standard stemming tool for English language text\n",
    "- smaller vocabulary: increase speed of execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9860c850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL url\n",
      "have have\n",
      "a a\n",
      "IS IS\n",
      "_ _\n",
      "NUMBER number\n",
      "_ _\n",
      "AWD AWD\n",
      "from from\n",
      "_ _\n",
      "NUMBER number\n",
      "_ _\n",
      "About about\n",
      "73 73\n",
      "K k\n",
      "miles mile\n",
      "on on\n",
      "it it\n",
      "I I\n",
      "ve ve\n",
      "never never\n",
      "touched touch\n",
      "the the\n",
      "engine engine\n",
      "radiator radiator\n",
      "coolant coolant\n",
      "and and\n",
      "can can\n",
      "t t\n",
      "find find\n",
      "anything anything\n",
      "on on\n",
      "when when\n",
      "to to\n",
      "change change\n",
      "this this\n",
      "in in\n",
      "the the\n",
      "book book\n",
      "It it\n",
      "just just\n",
      "says say\n",
      "long long\n",
      "life life\n",
      "100k 100k\n",
      "Toyota Toyota\n",
      "coolant coolant\n",
      "Does do\n",
      "anyone anyone\n",
      "get get\n",
      "this this\n",
      "flushed flushed\n",
      "or or\n",
      "changed change\n",
      "at at\n",
      "ten ten\n",
      "years year\n",
      "Do do\n",
      "I I\n",
      "wait wait\n",
      "until until\n",
      "100k 100k\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token,token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc7b4e8",
   "metadata": {},
   "source": [
    "### alternative syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a66acdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7c8bbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x17e19f3d970>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x17e19f3da30>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x17e0b375cb0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x17e1dafa750>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x17e1daf3310>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x17e0b375b60>)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69c6166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_alt = nlp(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07fa32e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(62 tokens: \"URL have a IS _NUMBER_ AWD from _NUMBER_ About ...\")'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_alt._.preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d5b77",
   "metadata": {},
   "source": [
    "### Analysis of a Doc\n",
    "\n",
    "- extracting n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24c99649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[73K,\n",
       " K miles,\n",
       " engine radiator,\n",
       " radiator coolant,\n",
       " t find,\n",
       " says long,\n",
       " long life,\n",
       " life 100k,\n",
       " 100k Toyota,\n",
       " Toyota coolant]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textacy import extract\n",
    "list(extract.ngrams(doc,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b73a81",
   "metadata": {},
   "source": [
    "- Identifying key terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bd361b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('engine radiator coolant', 0.0653202474980594),\n",
       " ('Toyota coolant', 0.04838537088251609),\n",
       " ('long life', 0.036703850291551265),\n",
       " ('k mile', 0.036327644669932505),\n",
       " ('AWD', 0.01817422855742821)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract.keyterms.textrank(doc, normalize=\"lemma\",topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4027b4",
   "metadata": {},
   "source": [
    "### Remark: We can discard some function of the spaCy pipeline\n",
    "\n",
    "We can import selected elements of the pipeline if some component are useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4021ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_2=spacy.load('en_core_web_sm', disable=[\"parser\",\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea7f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Customizing Tokenization\n",
    "\n",
    "Sometimes, it is necessary to adjust the Tokenizer to take into account hyphen, underscore, hash sign #, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e9d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"@Pete: can't choose low-carb # food #eat-smart. _url_ ; -) \"\n",
    "doc = nlp.make_doc(text)\n",
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43245c01",
   "metadata": {},
   "source": [
    "## Working with stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21685c1",
   "metadata": {},
   "source": [
    "- spaCy uses language-specific stop word lists to set the is_stop property for each token\n",
    "- Filtering stop words (and punctuation tokens) is easy\n",
    "- The list of stop words is loaded when a nlp object is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68b3d1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'where', 'just', 'into', 'so', 'it', 'did', '’ll', 'around', 'why', 'thereupon', 'four', 'must', 'nor', '‘ve', 'own', 'nevertheless', 'eleven', 'until', 'two', 'top', 'ours', 'often', 'due', 'thru', 'take', 'when', 'throughout', 'forty', 'anything', 'whatever', 'yet', 'could', 'several', 'was', 'indeed', 'each', 'last', '’d', 'that', 'a', 'done', 'least', '‘s', 'never', 'seeming', 'seem', 'go', 'less', 'any', 'are', 'else', 'perhaps', 'of', 'between', 'hence', 'such', 'therefore', 'against', 'always', 'anywhere', 'us', 'although', 'using', 'moreover', 'would', 'no', \"n't\", 'anyway', 'hers', 'too', 'beyond', 'already', 'toward', 'if', 'and', 'becoming', 'in', 'next', 'the', 'whether', 'sixty', 'he', 'however', 'him', 'twenty', 'sometime', 'they', 'same', 'also', 'formerly', 'amongst', 'before', '‘re', '’m', 'towards', 'everyone', 'again', \"'ve\", 'as', 're', 'amount', 'per', 'off', 'among', 'under', 'namely', 'once', 'mostly', 'yours', 'anyhow', 'together', 'because', 'neither', 'whence', 'mine', 'another', 'nothing', 'hereupon', 'n‘t', 'even', \"'m\", 'an', 'every', 'across', 'had', 'yourselves', '‘d', 'part', 'ten', 'put', 'above', 'herself', 'elsewhere', 'his', 'thence', 'about', 'below', 'seemed', 'now', 'during', 'over', 'up', \"'re\", 'something', 'for', 'nowhere', 'none', 'many', 'see', 'them', 'beside', 'is', 'while', 'does', 'itself', 'whoever', 'ever', '’re', 'empty', 'seems', 'call', 'her', 'doing', 'whom', 'whereas', \"'s\", 'some', 'along', 'through', 'might', 'regarding', 'unless', 'then', 'noone', 'ca', 'third', 'you', 'make', 'not', 'one', 'latterly', 'to', 'get', 'more', 'few', 'without', 'wherever', 'ourselves', 'what', 'eight', 'back', '‘ll', 'became', 'somewhere', 'has', 'whenever', 'keep', 'become', 'much', 'former', 'we', 'behind', 'or', 'made', 'fifteen', 'six', 'anyone', 'there', 'may', 'thereby', 'within', 'its', 'otherwise', 'thus', 'their', 'twelve', 'someone', 'out', 'i', 'whither', 'all', \"'ll\", 'this', 'say', 'but', 'our', 'onto', 'either', 'thereafter', 'rather', 'various', 'really', 'whereby', 'nine', 'latter', 'both', 'others', 'hereby', '’s', 'should', 'hereafter', 'everywhere', 'wherein', \"'d\", 'full', 'other', 'with', 'alone', 'themselves', 'very', 'via', 'my', 'himself', 'almost', 'your', 'n’t', 'further', 'quite', 'she', 'cannot', 'whereupon', 'therein', 'except', 'three', 'besides', '‘m', 'how', 'only', 'from', 'herein', 'meanwhile', 'yourself', 'somehow', 'whole', 'fifty', 'who', 'nobody', 'me', 'well', 'bottom', 'whose', 'can', 'which', 'will', 'on', 'give', 'afterwards', 'were', 'since', 'am', 'by', 'though', 'at', 'than', 'being', 'myself', 'everything', 'name', 'please', '’ve', 'after', 'show', 'used', 'move', 'front', 'these', 'do', 'sometimes', 'enough', 'here', 'serious', 'first', 'beforehand', 'whereafter', 'hundred', 'those', 'becomes', 'have', 'upon', 'been', 'most', 'be', 'down', 'still', 'side', 'five'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c59a80",
   "metadata": {},
   "source": [
    "### The list of stop words can be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67add01",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['down'].is_stop=False\n",
    "nlp.vocab['Dear'].is_stop=True\n",
    "nlp.vocab['Regards'].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d112c0",
   "metadata": {},
   "source": [
    "### Selection of words according to part-of-speech tags\n",
    "\n",
    "- Each token in a spaCy doc has two part-of-speech attributes:\n",
    "    - pos_\n",
    "    - tag_\n",
    "- tag_ can be language specific \n",
    "- pos_ contains the simplified tag of the universal part-of-speech tagset\n",
    "\n",
    "    https://spacy.io/usage/linguistic-features\n",
    "\n",
    "- pos_ can be used as an alternative to stop words\n",
    "- pronouns, prepositions, conjunctions, determiners: \n",
    "    - called **function words**\n",
    "    - their main function is to create grammatical relationships in a sentence\n",
    "    - not very informative\n",
    "\n",
    "- nouns, verbs, adjectives and adverbs: \n",
    "    - **content** words\n",
    "    - the meaning of a sentence depends on them\n",
    "    \n",
    "\n",
    "- We can **part-of-speech tags** to select the word types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2baa5ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[URL, IS, NUMBER, AWD, _, NUMBER, _, K, miles, engine, radiator, coolant, t, book, life, Toyota, coolant, years]\n"
     ]
    }
   ],
   "source": [
    "nouns=[t for t in doc if t.pos_ in ['NOUN','PROPN']]\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ef6bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'input doit être un objet de type doc\n",
    "tokens=textacy.extract.words(doc, include_pos={\"ADJ\",\"NOUN\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6cc3cda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL|NUMBER|NUMBER|K|miles|engine|radiator|coolant|t|book|long|life|coolant|flushed|years\n"
     ]
    }
   ],
   "source": [
    "print(*[t for t in tokens], sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "51b7f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'input doit être un objet de type doc\n",
    "tokens=textacy.extract.words(doc, include_pos={\"ADJ\",\"NOUN\"},min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*[t for t in tokens], sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lemmas(doc,**kwargs):\n",
    "    return[t.lemma_ for t in textacy.extract.words(doc,**kwargs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = extract_lemmas(doc,  include_pos={\"ADJ\",\"NOUN\"})\n",
    "print(*lemmas, sep = \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46796b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting Named entities\n",
    "\n",
    "- The process of detecting entities such as people, locations, organization in texts\n",
    "- In the **Named-entity recognizer** attributes of Doc:\n",
    "    - Doc.ents\n",
    "    - Token.ent_iob_\n",
    "    - Token.ent_type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"James O'Neill, chairman of World Cargo Inc, lives in San Francisco\"\n",
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(textacy.extract.entities(doc, include_types={\"PERSON\",\"ORG\",\"LOCATION\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49882074",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f\"({ent.text},{ent.label_})\",end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdf304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc,style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e853828c",
   "metadata": {},
   "source": [
    "# Make a Corpus\n",
    "\n",
    "A textacy.Corpus is an ordered collection of spaCy Doc all processed by the same language pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1798330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "records=df['text']\n",
    "\n",
    "preproc_records=((preproc(text)) for text in records)\n",
    "\n",
    "corpus=textacy.Corpus(\"en_core_web_sm\",data=preproc_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa4a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=textacy.Corpus(nlp,df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30e71825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(111 tokens: \"Looking for some help I ve never owned any luxu...\")'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[-1]._.preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b40d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.n_docs, corpus.n_sents, corpus.n_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
