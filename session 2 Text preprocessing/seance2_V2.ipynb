{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ec9e8cb-d07e-4a8c-8a53-69c11864d69a",
   "metadata": {},
   "source": [
    "# Preparing textual data for statistics and machine learning\n",
    "\n",
    "The purpose of this session is to use Python specialized libraries to prepare a sample of text for a subsequent quantitative analysis, for instance text classification. \n",
    "The differents steps of the process are:\n",
    "\n",
    "1. Importing the dataset\n",
    "2. Cleaning the dataset\n",
    "3. Tokenization\n",
    "4. Feature extraction on a large dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362cb11-fc85-4587-85d5-5c5c48ed2a8e",
   "metadata": {},
   "source": [
    "##  Data\n",
    "\n",
    "We use data of the reddit self-post classification task on Kaggle (https://www.kaggle.com/datasets/mswarbrickjones/reddit-selfposts)\n",
    "\n",
    "Reddit (https://www.reddit.com/) is a social media website.  A subreddit is a specific online community, and the posts associated with it. \n",
    "\n",
    "Subreddits are dedicated to a particular topic that people write about, and they're denoted by /r/, followed by the subreddit's name, e.g., /r/gaming.\n",
    "\n",
    "We have two datasets:\n",
    "\n",
    "1. **rspct.tsv**\n",
    "\n",
    "This dataset consists of 1.013M self-posts, posted from 1013 subreddits (1000 examples per class). \n",
    "\n",
    "For each post we give:\n",
    "- the subreddit, \n",
    "- the title,\n",
    "- the content of the self-post.\n",
    "\n",
    "On this file, observations are separated by a tab\n",
    "\n",
    "\n",
    "2. **subreddit_info.csv**\n",
    "\n",
    "Contains manual annotation of about 3000 subreddits :\n",
    "\n",
    "- a top-level category and subcategory for each subreddit, \n",
    "\n",
    "- a reason for exclusion if this does not appear in the data.\n",
    "\n",
    "\n",
    "As a first step, we will:\n",
    "\n",
    "- Import these two datasets\n",
    "- Make a joint dataframe between these two dataframe based on the subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4506e871-57cb-4f0c-b679-f0e795b49950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c632ab22-6b22-4ccd-8ce4-58f784923010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1013000 entries, 0 to 1012999\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   id         1013000 non-null  object\n",
      " 1   subreddit  1013000 non-null  object\n",
      " 2   title      1013000 non-null  object\n",
      " 3   selftext   1013000 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 30.9+ MB\n"
     ]
    }
   ],
   "source": [
    "posts_file = \"rspct.tsv\"\n",
    "\n",
    "posts_df = pd.read_csv(posts_file, sep='\\t')\n",
    "\n",
    "posts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec9b2076-99dd-4e71-84da-9973474ab9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1013000, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ace1fc76-2c3e-4271-932f-1ff1fef40465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6d8knd</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58mbft</td>\n",
       "      <td>teenmom</td>\n",
       "      <td>So what was Matt \"addicted\" to?</td>\n",
       "      <td>Did he ever say what his addiction was or is h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6ti6re</td>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>Not door bell, but floodlight mount height.</td>\n",
       "      <td>I know this is a sub for the 'Ring Doorbell' b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77sxto</td>\n",
       "      <td>intel</td>\n",
       "      <td>Worried about my 8700k small fft/data stress r...</td>\n",
       "      <td>Prime95 (regardless of version) and OCCT both,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5qw3x0</td>\n",
       "      <td>residentevil</td>\n",
       "      <td>What if Saddler won?</td>\n",
       "      <td>I just wanted to start a thread about what wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7jve7p</td>\n",
       "      <td>BATProject</td>\n",
       "      <td>Net Neutrality and Brave</td>\n",
       "      <td>If and when net neutrality laws are repealed i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6icvfu</td>\n",
       "      <td>hockeyplayers</td>\n",
       "      <td>Inline Hockey: Where Do I Need To Be? (Positio...</td>\n",
       "      <td>My game is coming on well but one HUGE aspect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4y7c5c</td>\n",
       "      <td>asmr</td>\n",
       "      <td>[Question] Who is your favorite defunct ASMRtist?</td>\n",
       "      <td>\"Defunct\" being defined here as NOT having rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6azhj1</td>\n",
       "      <td>rawdenim</td>\n",
       "      <td>Had a custom embroidery job done on my ranch j...</td>\n",
       "      <td>[Album First](http://imgur.com/a/DYdKC)&lt;lb&gt;&lt;lb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             subreddit  \\\n",
       "0  6d8knd  talesfromtechsupport   \n",
       "1  58mbft               teenmom   \n",
       "2  8f73s7                Harley   \n",
       "3  6ti6re          ringdoorbell   \n",
       "4  77sxto                 intel   \n",
       "5  5qw3x0          residentevil   \n",
       "6  7jve7p            BATProject   \n",
       "7  6icvfu         hockeyplayers   \n",
       "8  4y7c5c                  asmr   \n",
       "9  6azhj1              rawdenim   \n",
       "\n",
       "                                               title  \\\n",
       "0             Remember your command line switches...   \n",
       "1                    So what was Matt \"addicted\" to?   \n",
       "2                                     No Club Colors   \n",
       "3        Not door bell, but floodlight mount height.   \n",
       "4  Worried about my 8700k small fft/data stress r...   \n",
       "5                               What if Saddler won?   \n",
       "6                           Net Neutrality and Brave   \n",
       "7  Inline Hockey: Where Do I Need To Be? (Positio...   \n",
       "8  [Question] Who is your favorite defunct ASMRtist?   \n",
       "9  Had a custom embroidery job done on my ranch j...   \n",
       "\n",
       "                                            selftext  \n",
       "0  Hi there,  <lb>The usual. Long time lerker, fi...  \n",
       "1  Did he ever say what his addiction was or is h...  \n",
       "2  Funny story. I went to college in Las Vegas. T...  \n",
       "3  I know this is a sub for the 'Ring Doorbell' b...  \n",
       "4  Prime95 (regardless of version) and OCCT both,...  \n",
       "5  I just wanted to start a thread about what wou...  \n",
       "6  If and when net neutrality laws are repealed i...  \n",
       "7  My game is coming on well but one HUGE aspect ...  \n",
       "8  \"Defunct\" being defined here as NOT having rel...  \n",
       "9  [Album First](http://imgur.com/a/DYdKC)<lb><lb...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55efb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of subreddit\n",
    "posts_df['subreddit'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79afb15-ec81-461c-a2f4-f3f13cc6f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=posts_df['subreddit']=='whatsthatbook'\n",
    "posts_df.loc[mask,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a8dad-f3ac-420f-a9c1-bef5b1ea48f9",
   "metadata": {},
   "source": [
    "**subreddit_info.csv**\n",
    "\n",
    "Contains manual annotation of about 3000 subreddits :\n",
    "    \n",
    "    - a top-level category and subcategory for each subreddit, \n",
    "    \n",
    "    - a reason for exclusion if this does not appear in the data.\n",
    "\n",
    "These information can be considerered as  **metadata**: information on characteristics of the text (and not the content of the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50604f50-6368-47fc-a259-a5dccc477120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3394 entries, 0 to 3393\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   subreddit             3394 non-null   object\n",
      " 1   category_1            3394 non-null   object\n",
      " 2   category_2            3362 non-null   object\n",
      " 3   category_3            536 non-null    object\n",
      " 4   in_data               3394 non-null   bool  \n",
      " 5   reason_for_exclusion  2381 non-null   object\n",
      "dtypes: bool(1), object(5)\n",
      "memory usage: 136.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>whatsthatbook</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>book</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CasualConversation</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clairvoyantreadings</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecidingToBeBetter</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HelpMeFind</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLPLounge</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NoStupidQuestions</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RBI</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit       category_1 category_2 category_3  in_data  \\\n",
       "0        whatsthatbook  advice/question       book        NaN     True   \n",
       "1   CasualConversation  advice/question      broad        NaN    False   \n",
       "2  Clairvoyantreadings  advice/question      broad        NaN    False   \n",
       "3   DecidingToBeBetter  advice/question      broad        NaN    False   \n",
       "4           HelpMeFind  advice/question      broad        NaN    False   \n",
       "5          LifeProTips  advice/question      broad        NaN    False   \n",
       "6            MLPLounge  advice/question      broad        NaN    False   \n",
       "7    NoStupidQuestions  advice/question      broad        NaN    False   \n",
       "8                  RBI  advice/question      broad        NaN    False   \n",
       "9       TooAfraidToAsk  advice/question      broad        NaN    False   \n",
       "\n",
       "  reason_for_exclusion  \n",
       "0                  NaN  \n",
       "1            too_broad  \n",
       "2            too_broad  \n",
       "3            too_broad  \n",
       "4            too_broad  \n",
       "5            too_broad  \n",
       "6            too_broad  \n",
       "7            too_broad  \n",
       "8            too_broad  \n",
       "9            too_broad  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subred_file = \"subreddit_info.csv\"\n",
    "subred_df=pd.read_csv(subred_file)\n",
    "subred_df.info()\n",
    "subred_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8efa161-55ea-4fb1-8083-56a5da03c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_file = \"subreddit_info.csv\"\n",
    "subred_df=pd.read_csv(subred_file).set_index(['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b480472-8b71-484a-a346-0950890dfb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3394, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subred_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3126d21-3b74-43bb-816b-ebe1f4889002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>whatsthatbook</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>book</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CasualConversation</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clairvoyantreadings</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecidingToBeBetter</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HelpMeFind</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LifeProTips</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPLounge</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NoStupidQuestions</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBI</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TooAfraidToAsk</th>\n",
       "      <td>advice/question</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          category_1 category_2 category_3  in_data  \\\n",
       "subreddit                                                             \n",
       "whatsthatbook        advice/question       book        NaN     True   \n",
       "CasualConversation   advice/question      broad        NaN    False   \n",
       "Clairvoyantreadings  advice/question      broad        NaN    False   \n",
       "DecidingToBeBetter   advice/question      broad        NaN    False   \n",
       "HelpMeFind           advice/question      broad        NaN    False   \n",
       "LifeProTips          advice/question      broad        NaN    False   \n",
       "MLPLounge            advice/question      broad        NaN    False   \n",
       "NoStupidQuestions    advice/question      broad        NaN    False   \n",
       "RBI                  advice/question      broad        NaN    False   \n",
       "TooAfraidToAsk       advice/question      broad        NaN    False   \n",
       "\n",
       "                    reason_for_exclusion  \n",
       "subreddit                                 \n",
       "whatsthatbook                        NaN  \n",
       "CasualConversation             too_broad  \n",
       "Clairvoyantreadings            too_broad  \n",
       "DecidingToBeBetter             too_broad  \n",
       "HelpMeFind                     too_broad  \n",
       "LifeProTips                    too_broad  \n",
       "MLPLounge                      too_broad  \n",
       "NoStupidQuestions              too_broad  \n",
       "RBI                            too_broad  \n",
       "TooAfraidToAsk                 too_broad  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subred_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abad72-5c92-4ad8-b5e8-86960c0eff4a",
   "metadata": {},
   "source": [
    "## Joining the two dataframes ##\n",
    "\n",
    "We want to gather the two previous datasets, on the basis of the subreddit which is a column of posts_df and the index of subred_df. \n",
    "\n",
    "subreddit : column in the caller (posts_df) to join on the index of subred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c329eeb7-a88f-4cca-b4a5-ac81d7594dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=posts_df.join(subred_df, on ='subreddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "239bf600-5864-4b67-a052-841da3a069ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1013000 entries, 0 to 1012999\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count    Dtype \n",
      "---  ------                --------------    ----- \n",
      " 0   id                    1013000 non-null  object\n",
      " 1   subreddit             1013000 non-null  object\n",
      " 2   title                 1013000 non-null  object\n",
      " 3   selftext              1013000 non-null  object\n",
      " 4   category_1            1013000 non-null  object\n",
      " 5   category_2            1013000 non-null  object\n",
      " 6   category_3            136000 non-null   object\n",
      " 7   in_data               1013000 non-null  bool  \n",
      " 8   reason_for_exclusion  0 non-null        object\n",
      "dtypes: bool(1), object(8)\n",
      "memory usage: 62.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9fe74f78-11c3-4df9-9c5d-d90d73644b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1013000, 9)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7d298d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            0\n",
       "subreddit                     0\n",
       "title                         0\n",
       "selftext                      0\n",
       "category_1                    0\n",
       "category_2                    0\n",
       "category_3               877000\n",
       "in_data                       0\n",
       "reason_for_exclusion    1013000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fbb6f-13de-4013-9d81-6b7055ae1b1e",
   "metadata": {},
   "source": [
    "### Standardizing Attributes Names\n",
    "\n",
    "Usual practise:\n",
    "- **df**: name of the dataset\n",
    "- **text**: name of the column containing text to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a7259734-9e5d-4a13-8489-bea048bf1703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'subreddit', 'title', 'selftext', 'category_1', 'category_2',\n",
      "       'category_3', 'in_data', 'reason_for_exclusion'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95a2c48d-a5bb-4a5f-a9f7-7005f22cda63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['category_3', 'in_data', 'reason_for_exclusion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a9b4474-a858-4a7d-ac55-e541d6b4b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    'id':'id',\n",
    "    'subreddit':'subreddit',\n",
    "    'title':'title',\n",
    "    'selftext':'text',\n",
    "    'category_1':'category',\n",
    "    'category_2':'subcategory',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c05077c1-ba20-4d86-91ab-f9cb27b0379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    'selftext':'text',\n",
    "    'category_1':'category',\n",
    "    'category_2':'subcategory',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ed921e5-792f-4689-b045-11b269435224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'subreddit', 'title', 'text', 'category', 'subcategory'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df=df.rename(columns=column_mapping)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4390aca-456c-4f54-a569-d18f2cc4f111",
   "metadata": {},
   "source": [
    "#### Renaming columns and suppressing NaN columns - alternative method\n",
    "\n",
    "- selftext renamed as text\n",
    "- category_1 renamed as category\n",
    "- category_2 renamed as subcategory\n",
    "\n",
    "\n",
    " category_3, in_data and reason_for_exclusion **are suppressed (incomplete data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339721b-0800-454c-aab2-f4235f22c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    'id':'id',\n",
    "    'subreddit':'subreddit',\n",
    "    'title':'title',\n",
    "    'selftext':'text',\n",
    "    'category_1':'category',\n",
    "    'category_2':'subcategory',\n",
    "    'category_3': None,\n",
    "    'in_data': None,\n",
    "    'reason_for_exclusion': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4d0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941a533-a395-4e03-a071-9eb6dc84412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[c for c in column_mapping.keys() if column_mapping[c] != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f18a66-26c3-45f9-9a77-78f1a272bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baea0af-8b87-4f01-846d-7da9cf0e8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[columns].rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b677be-1db6-42e5-9d65-c24ab87b314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed08a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d00aa3-ffcd-4ebc-925f-77453945f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['category'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdcbafb-0e18-4729-b48d-81d85bf790a1",
   "metadata": {},
   "source": [
    "### Selection of data for the autos category\n",
    "\n",
    "We restrict the data to the autos category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1910bbb4-128c-4f9d-8f35-dbe11ca5f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['category']=='autos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee265aa4-d6af-4ddb-879c-69fe174680d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20000 entries, 2 to 1012979\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           20000 non-null  object\n",
      " 1   subreddit    20000 non-null  object\n",
      " 2   title        20000 non-null  object\n",
      " 3   text         20000 non-null  object\n",
      " 4   category     20000 non-null  object\n",
      " 5   subcategory  20000 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61389955-b00a-457e-a9bd-f674f7a5b78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "      <td>autos</td>\n",
       "      <td>harley davidson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>5s0q8r</td>\n",
       "      <td>Mustang</td>\n",
       "      <td>Roush vs Shleby GT500</td>\n",
       "      <td>I am trying to determine which is faster, and ...</td>\n",
       "      <td>autos</td>\n",
       "      <td>ford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>5z3405</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>2001 Golf Wagon looking for some insight</td>\n",
       "      <td>Hello! &lt;lb&gt;&lt;lb&gt;Trying to find some information...</td>\n",
       "      <td>autos</td>\n",
       "      <td>VW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>7df18v</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>IS 250 Coolant Flush/Change</td>\n",
       "      <td>https://www.cars.com/articles/how-often-should...</td>\n",
       "      <td>autos</td>\n",
       "      <td>lexus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>5tpve8</td>\n",
       "      <td>volt</td>\n",
       "      <td>Gen1 mpg w/ dead battery?</td>\n",
       "      <td>Hi, new to this subreddit.  I'm considering bu...</td>\n",
       "      <td>autos</td>\n",
       "      <td>chevrolet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   subreddit                                     title  \\\n",
       "2    8f73s7      Harley                            No Club Colors   \n",
       "56   5s0q8r     Mustang                     Roush vs Shleby GT500   \n",
       "78   5z3405  Volkswagen  2001 Golf Wagon looking for some insight   \n",
       "270  7df18v       Lexus               IS 250 Coolant Flush/Change   \n",
       "286  5tpve8        volt                 Gen1 mpg w/ dead battery?   \n",
       "\n",
       "                                                  text category  \\\n",
       "2    Funny story. I went to college in Las Vegas. T...    autos   \n",
       "56   I am trying to determine which is faster, and ...    autos   \n",
       "78   Hello! <lb><lb>Trying to find some information...    autos   \n",
       "270  https://www.cars.com/articles/how-often-should...    autos   \n",
       "286  Hi, new to this subreddit.  I'm considering bu...    autos   \n",
       "\n",
       "         subcategory  \n",
       "2    harley davidson  \n",
       "56              ford  \n",
       "78                VW  \n",
       "270            lexus  \n",
       "286        chevrolet  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aadc135f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49f77b-71e1-45b1-a1b0-57e245925ab7",
   "metadata": {},
   "source": [
    "## Python libraries\n",
    "\n",
    "Two associated Python libraries:\n",
    "\n",
    "**textacy**(https://pypi.org/project/textacy/)\n",
    "\n",
    "        preprocessing = clean, normalize and explore raw data before processing it with spaCy*\n",
    "        \n",
    "**spaCy** (https://spacy.io/)\n",
    "            \n",
    "        fundamentals = tokenization, part-of-speech tagging, dependency parsing..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d4c4ed-18fe-4b0f-a316-8c8af2492bd7",
   "metadata": {},
   "source": [
    "## Preliminary step: Cleaning Text Data with textacy\n",
    "\n",
    "We don't have well edited texts. There are several problems of quality that we need to take into account:\n",
    "\n",
    "- **Salutations, signatures and adresses**: usually not informative\n",
    "    \n",
    "\n",
    "- **Replies**: in case the text contains replies repeating the question, we need to eliminate the duplicated question. If not, we can introduce bias in the statistical analysis.\n",
    "    \n",
    "    \n",
    "- **Special formatting and program code**: in case, the text contain special characters, HTML entities, Mardown tags,...Necessary to eliminate these signs before the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d046998",
   "metadata": {},
   "source": [
    "- TextaCy module used to perform (preliminary/cleaning) NLP tasks on texts:\n",
    "    \n",
    "    - replacing and removing punctuation, extra whitespaces, numbers from the text before processing with spaCy\n",
    "    \n",
    "- Built upon the SpaCy module in Python\n",
    "\n",
    "https://www.geeksforgeeks.org/textacy-module-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dbe3c70e-0c06-480f-809b-e9a320e1caf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([      2,      56,      78,     270,     286,     337,     361,     415,\n",
       "           502,     582,\n",
       "       ...\n",
       "       1012426, 1012455, 1012520, 1012552, 1012614, 1012634, 1012658, 1012859,\n",
       "       1012969, 1012979],\n",
       "      dtype='int64', length=20000)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "477ccbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling whatsoever. Me and some college buddies would always go out on the strip to the dance clubs. We always ended up at a bar called Hogs &amp; Heifers. It's worth noting the females working there can outdrink ANYONE. Anyway, there was a sign on the front door that read 'No Club Colors'. So we lose our ties and blazers before heading there. Also we assumed bright colors like red, yellow, green etc were not allowed. So we would always bring an xtra t-shirt and pair of jeans. This went on for years! Looking back now on how naive we were, it's just hilarious. I was never able to walk out of that bar....had to crawl out! So much booze. <lb><lb>Cheers. Ride safe, boys! \n"
     ]
    }
   ],
   "source": [
    "text=df.loc[df.index[0],'text'] # selection of text by using df.index[list]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1733fa9a-78a2-4cfe-a4f7-e7630dc6737c",
   "metadata": {},
   "source": [
    "Raw text sometimes needs to be cleaned before analysis\n",
    "\n",
    "textacy.preprocessing sub-package contains a number of functions:\n",
    "\n",
    "- to normalize (whitespace, quotation marks,...)\n",
    "\n",
    "- remove (punctuations, accents,...)\n",
    "\n",
    "- replace (URLs, emails, numbers, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "118b6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "import textacy.preprocessing as tprep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73372e0c-fb58-4dca-afba-906f724583ba",
   "metadata": {},
   "source": [
    "With make_pipeline, we make a callable pipeline which take a text as input, passes it through the functions in squential orders and then output a single preprocessed string text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00d38dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = tprep.make_pipeline(\n",
    "    tprep.normalize.hyphenated_words,\n",
    "    tprep.normalize.quotation_marks,\n",
    "    tprep.normalize.unicode,\n",
    "    tprep.normalize.whitespace,\n",
    "    tprep.remove.html_tags,\n",
    "    tprep.remove.accents,\n",
    "    tprep.remove.punctuation,\n",
    "    tprep.remove.brackets,\n",
    "    tprep.replace.numbers,\n",
    "    tprep.replace.urls,\n",
    "    tprep.replace.currency_symbols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47d63aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny story  I went to college in Las Vegas  This was before I knew anything about motorcycling whatsoever  Me and some college buddies would always go out on the strip to the dance clubs  We always ended up at a bar called Hogs   Heifers  It s worth noting the females working there can outdrink ANYONE  Anyway  there was a sign on the front door that read  No Club Colors   So we lose our ties and blazers before heading there  Also we assumed bright colors like red  yellow  green etc were not allowed  So we would always bring an xtra t shirt and pair of jeans  This went on for years  Looking back now on how naive we were  it s just hilarious  I was never able to walk out of that bar    had to crawl out  So much booze  Cheers  Ride safe  boys \n"
     ]
    }
   ],
   "source": [
    "clean_text=preproc(text)\n",
    "\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a10525c-dc07-452e-ba8c-608f088b75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2= 'There is (no) of these 10 examples of 100 £ loans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "addfdd4e-f51f-4d92-b197-4c4df19efcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is  no  of these _NUMBER_ examples of _NUMBER_ _CUR_ loans'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c80ab3",
   "metadata": {},
   "source": [
    "### Alternative: creating a specific function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09957002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = tprep.replace.urls(text)# we replace url with text\n",
    "    text = tprep.remove.html_tags(text)\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    text = tprep.remove.punctuation(text)\n",
    "    text = tprep.normalize.whitespace(text)\n",
    "    text = tprep.replace.numbers(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88c06cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny story I went to college in Las Vegas This was before I knew anything about motorcycling whatsoever Me and some college buddies would always go out on the strip to the dance clubs We always ended up at a bar called Hogs Heifers It s worth noting the females working there can outdrink ANYONE Anyway there was a sign on the front door that read No Club Colors So we lose our ties and blazers before heading there Also we assumed bright colors like red yellow green etc were not allowed So we would always bring an xtra t shirt and pair of jeans This went on for years Looking back now on how naive we were it s just hilarious I was never able to walk out of that bar had to crawl out So much booze Cheers Ride safe boys\n"
     ]
    }
   ],
   "source": [
    "print(normalize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51952682-c3e1-48ff-810b-8a519dc100c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df.loc[df.index[:5],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7db730e2-dd46-4437-8872-7f372ee9e987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5 entries, 2 to 286\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           5 non-null      object\n",
      " 1   subreddit    5 non-null      object\n",
      " 2   title        5 non-null      object\n",
      " 3   text         5 non-null      object\n",
      " 4   category     5 non-null      object\n",
      " 5   subcategory  5 non-null      object\n",
      "dtypes: object(6)\n",
      "memory usage: 280.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_small.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2592e49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2      Funny story I went to college in Las Vegas Thi...\n",
       "56     I am trying to determine which is faster and I...\n",
       "78     Hello Trying to find some information on repla...\n",
       "270    URL have a IS _NUMBER_ AWD from _NUMBER_ About...\n",
       "286    Hi new to this subreddit I m considering buyin...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small['text'].apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0fb518-de58-41d2-93e0-14453b224287",
   "metadata": {},
   "source": [
    "## Linguistic Processing with spaCy\n",
    "\n",
    "- Spacy: library for linguistic data processing\n",
    "\n",
    "- spaCy's pipeline is language dependent: we hav to load a particular pipeline to process the text \n",
    "    \n",
    "- Spacy provide an integrated pipeline of processing documents:\n",
    "    \n",
    "    1. a tokenizer (by default) : tok2vec\n",
    "    2. a part-of-speech tagger : tagger\n",
    "    3. a dependency parser : parser\n",
    "    4. a sentence recognizer : senter\n",
    "    5. a attribute ruler \n",
    "    6. a lemmatizer : lemmatizer\n",
    "    7. a named-entity recognizer : ner\n",
    "    \n",
    "- the tokenizes is based on language-dependent rules = > fast\n",
    "\n",
    "\n",
    "- 2, 3 and 4 are based on pretrained neural models => can 10-20 times as long as tokenization\n",
    "\n",
    "- The initial input is a text\n",
    "\n",
    "- The final output is a **Doc** object\n",
    "\n",
    "- The **Doc** object contains a list of **Tokens** objects\n",
    "\n",
    "- Any range selection of tokens creates a **Span**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4eea1c-b1b4-4c14-b35d-f26a02ec08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "We import spaCy one of trained pipelines for english \n",
    "\n",
    "For example, en_core_web_sm is a small English pipeline trained on was trained on an annotated corpus called “OntoNotes”: 2 million+ words drawn from “news, broadcast, talk shows, weblogs, usenet newsgroups, and conversational telephone speech,” which were meticulously tagged by a group of researchers and professionals for people’s names and places, for nouns and verbs, for subjects and objects, and much more.\n",
    "\n",
    "https://spacy.io/models/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca6c1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ac4328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 'en_core_wb_sm' is the name of the installed spaCy pipeline\n",
    "from spacy.cli import download\n",
    "print(download('en_core_web_sm'))\n",
    "#print(download('en_core_web_md'))\n",
    "#print(download('en_core_web_lg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ddc60c-5130-48c2-a1d5-0b9e1eb0dd36",
   "metadata": {},
   "source": [
    "We make a spaCy **Doc** from text\n",
    "\n",
    "A doc is required as inputs of the functions of spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea8206d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(166 tokens: \"Funny story  I went to college in Las Vegas  Th...\")'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = textacy.make_spacy_doc(clean_text,lang=\"en_core_web_sm\")\n",
    "doc._.preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2d6db4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny story  I went to college in Las Vegas  This was before I knew anything about motorcycling whatsoever  Me and some college buddies would always go out on the strip to the dance clubs  We always ended up at a bar called Hogs   Heifers  It s worth noting the females working there can outdrink ANYONE  Anyway  there was a sign on the front door that read  No Club Colors   So we lose our ties and blazers before heading there  Also we assumed bright colors like red  yellow  green etc were not allowed  So we would always bring an xtra t shirt and pair of jeans  This went on for years  Looking back now on how naive we were  it s just hilarious  I was never able to walk out of that bar    had to crawl out  So much booze  Cheers  Ride safe  boys \n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e098bb-f2ae-4123-8278-b96684f1fb22",
   "metadata": {},
   "source": [
    "### Alternative code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b596dbb-c185-477d-9199-e7fdc90763a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0c12670d-3ecf-4a6e-8ab9-5541724a58a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x24a23c7f170>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x24a23c7f350>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x24a23cb0200>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x24a21a28190>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x24a23ce2a90>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x24a23cb03c0>)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2d335588-7ea8-4e18-90a2-dd6697d4f18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny story  I went to college in Las Vegas  This was before I knew anything about motorcycling whatsoever  Me and some college buddies would always go out on the strip to the dance clubs  We always ended up at a bar called Hogs   Heifers  It s worth noting the females working there can outdrink ANYONE  Anyway  there was a sign on the front door that read  No Club Colors   So we lose our ties and blazers before heading there  Also we assumed bright colors like red  yellow  green etc were not allowed  So we would always bring an xtra t shirt and pair of jeans  This went on for years  Looking back now on how naive we were  it s just hilarious  I was never able to walk out of that bar    had to crawl out  So much booze  Cheers  Ride safe  boys \n"
     ]
    }
   ],
   "source": [
    "doc_alt = nlp(clean_text)\n",
    "print(doc_alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af1906",
   "metadata": {},
   "source": [
    "### Displaying tokens in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28683a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny\n",
      "story\n",
      " \n",
      "I\n",
      "went\n",
      "to\n",
      "college\n",
      "in\n",
      "Las\n",
      "Vegas\n",
      " \n",
      "This\n",
      "was\n",
      "before\n",
      "I\n",
      "knew\n",
      "anything\n",
      "about\n",
      "motorcycling\n",
      "whatsoever\n",
      " \n",
      "Me\n",
      "and\n",
      "some\n",
      "college\n",
      "buddies\n",
      "would\n",
      "always\n",
      "go\n",
      "out\n",
      "on\n",
      "the\n",
      "strip\n",
      "to\n",
      "the\n",
      "dance\n",
      "clubs\n",
      " \n",
      "We\n",
      "always\n",
      "ended\n",
      "up\n",
      "at\n",
      "a\n",
      "bar\n",
      "called\n",
      "Hogs\n",
      "  \n",
      "Heifers\n",
      " \n",
      "It\n",
      "s\n",
      "worth\n",
      "noting\n",
      "the\n",
      "females\n",
      "working\n",
      "there\n",
      "can\n",
      "outdrink\n",
      "ANYONE\n",
      " \n",
      "Anyway\n",
      " \n",
      "there\n",
      "was\n",
      "a\n",
      "sign\n",
      "on\n",
      "the\n",
      "front\n",
      "door\n",
      "that\n",
      "read\n",
      " \n",
      "No\n",
      "Club\n",
      "Colors\n",
      "  \n",
      "So\n",
      "we\n",
      "lose\n",
      "our\n",
      "ties\n",
      "and\n",
      "blazers\n",
      "before\n",
      "heading\n",
      "there\n",
      " \n",
      "Also\n",
      "we\n",
      "assumed\n",
      "bright\n",
      "colors\n",
      "like\n",
      "red\n",
      " \n",
      "yellow\n",
      " \n",
      "green\n",
      "etc\n",
      "were\n",
      "not\n",
      "allowed\n",
      " \n",
      "So\n",
      "we\n",
      "would\n",
      "always\n",
      "bring\n",
      "an\n",
      "xtra\n",
      "t\n",
      "shirt\n",
      "and\n",
      "pair\n",
      "of\n",
      "jeans\n",
      " \n",
      "This\n",
      "went\n",
      "on\n",
      "for\n",
      "years\n",
      " \n",
      "Looking\n",
      "back\n",
      "now\n",
      "on\n",
      "how\n",
      "naive\n",
      "we\n",
      "were\n",
      " \n",
      "it\n",
      "s\n",
      "just\n",
      "hilarious\n",
      " \n",
      "I\n",
      "was\n",
      "never\n",
      "able\n",
      "to\n",
      "walk\n",
      "out\n",
      "of\n",
      "that\n",
      "bar\n",
      "   \n",
      "had\n",
      "to\n",
      "crawl\n",
      "out\n",
      " \n",
      "So\n",
      "much\n",
      "booze\n",
      " \n",
      "Cheers\n",
      " \n",
      "Ride\n",
      "safe\n",
      " \n",
      "boys\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0abf3-1f98-4b29-9c7b-7c3c1023cd56",
   "metadata": {},
   "source": [
    "### Tokens have attributes \n",
    "\n",
    "    - token.is_punct  : Is the token punctuation? \n",
    "    - token.is_alpha  : Does the token consist of alphabetic characters? \n",
    "    - token.like_email : Does the token resemble an email address?\n",
    "    - token.like_url : : Does the token resemble a URL?\n",
    "\n",
    "    - token.is_stop : Is the token part of a “stop list”?\n",
    "    - token.lemma_ : Base form of the token, with no inflectional suffixes.\n",
    "    - token.pos : core part-of-speech categories https://universaldependencies.org/u/pos/\n",
    "            \n",
    "            \n",
    "See https://spacy.io/api/token for the list of all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bcc04fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny False\n",
      "story False\n",
      "  False\n",
      "I False\n",
      "went False\n",
      "to False\n",
      "college False\n",
      "in False\n",
      "Las False\n",
      "Vegas False\n",
      "  False\n",
      "This False\n",
      "was False\n",
      "before False\n",
      "I False\n",
      "knew False\n",
      "anything False\n",
      "about False\n",
      "motorcycling False\n",
      "whatsoever False\n",
      "  False\n",
      "Me False\n",
      "and False\n",
      "some False\n",
      "college False\n",
      "buddies False\n",
      "would False\n",
      "always False\n",
      "go False\n",
      "out False\n",
      "on False\n",
      "the False\n",
      "strip False\n",
      "to False\n",
      "the False\n",
      "dance False\n",
      "clubs False\n",
      "  False\n",
      "We False\n",
      "always False\n",
      "ended False\n",
      "up False\n",
      "at False\n",
      "a False\n",
      "bar False\n",
      "called False\n",
      "Hogs False\n",
      "   False\n",
      "Heifers False\n",
      "  False\n",
      "It False\n",
      "s False\n",
      "worth False\n",
      "noting False\n",
      "the False\n",
      "females False\n",
      "working False\n",
      "there False\n",
      "can False\n",
      "outdrink False\n",
      "ANYONE False\n",
      "  False\n",
      "Anyway False\n",
      "  False\n",
      "there False\n",
      "was False\n",
      "a False\n",
      "sign False\n",
      "on False\n",
      "the False\n",
      "front False\n",
      "door False\n",
      "that False\n",
      "read False\n",
      "  False\n",
      "No False\n",
      "Club False\n",
      "Colors False\n",
      "   False\n",
      "So False\n",
      "we False\n",
      "lose False\n",
      "our False\n",
      "ties False\n",
      "and False\n",
      "blazers False\n",
      "before False\n",
      "heading False\n",
      "there False\n",
      "  False\n",
      "Also False\n",
      "we False\n",
      "assumed False\n",
      "bright False\n",
      "colors False\n",
      "like False\n",
      "red False\n",
      "  False\n",
      "yellow False\n",
      "  False\n",
      "green False\n",
      "etc False\n",
      "were False\n",
      "not False\n",
      "allowed False\n",
      "  False\n",
      "So False\n",
      "we False\n",
      "would False\n",
      "always False\n",
      "bring False\n",
      "an False\n",
      "xtra False\n",
      "t False\n",
      "shirt False\n",
      "and False\n",
      "pair False\n",
      "of False\n",
      "jeans False\n",
      "  False\n",
      "This False\n",
      "went False\n",
      "on False\n",
      "for False\n",
      "years False\n",
      "  False\n",
      "Looking False\n",
      "back False\n",
      "now False\n",
      "on False\n",
      "how False\n",
      "naive False\n",
      "we False\n",
      "were False\n",
      "  False\n",
      "it False\n",
      "s False\n",
      "just False\n",
      "hilarious False\n",
      "  False\n",
      "I False\n",
      "was False\n",
      "never False\n",
      "able False\n",
      "to False\n",
      "walk False\n",
      "out False\n",
      "of False\n",
      "that False\n",
      "bar False\n",
      "    False\n",
      "had False\n",
      "to False\n",
      "crawl False\n",
      "out False\n",
      "  False\n",
      "So False\n",
      "much False\n",
      "booze False\n",
      "  False\n",
      "Cheers False\n",
      "  False\n",
      "Ride False\n",
      "safe False\n",
      "  False\n",
      "boys False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token,token.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27fd334f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny True\n",
      "story True\n",
      "  False\n",
      "I True\n",
      "went True\n",
      "to True\n",
      "college True\n",
      "in True\n",
      "Las True\n",
      "Vegas True\n",
      "  False\n",
      "This True\n",
      "was True\n",
      "before True\n",
      "I True\n",
      "knew True\n",
      "anything True\n",
      "about True\n",
      "motorcycling True\n",
      "whatsoever True\n",
      "  False\n",
      "Me True\n",
      "and True\n",
      "some True\n",
      "college True\n",
      "buddies True\n",
      "would True\n",
      "always True\n",
      "go True\n",
      "out True\n",
      "on True\n",
      "the True\n",
      "strip True\n",
      "to True\n",
      "the True\n",
      "dance True\n",
      "clubs True\n",
      "  False\n",
      "We True\n",
      "always True\n",
      "ended True\n",
      "up True\n",
      "at True\n",
      "a True\n",
      "bar True\n",
      "called True\n",
      "Hogs True\n",
      "   False\n",
      "Heifers True\n",
      "  False\n",
      "It True\n",
      "s True\n",
      "worth True\n",
      "noting True\n",
      "the True\n",
      "females True\n",
      "working True\n",
      "there True\n",
      "can True\n",
      "outdrink True\n",
      "ANYONE True\n",
      "  False\n",
      "Anyway True\n",
      "  False\n",
      "there True\n",
      "was True\n",
      "a True\n",
      "sign True\n",
      "on True\n",
      "the True\n",
      "front True\n",
      "door True\n",
      "that True\n",
      "read True\n",
      "  False\n",
      "No True\n",
      "Club True\n",
      "Colors True\n",
      "   False\n",
      "So True\n",
      "we True\n",
      "lose True\n",
      "our True\n",
      "ties True\n",
      "and True\n",
      "blazers True\n",
      "before True\n",
      "heading True\n",
      "there True\n",
      "  False\n",
      "Also True\n",
      "we True\n",
      "assumed True\n",
      "bright True\n",
      "colors True\n",
      "like True\n",
      "red True\n",
      "  False\n",
      "yellow True\n",
      "  False\n",
      "green True\n",
      "etc True\n",
      "were True\n",
      "not True\n",
      "allowed True\n",
      "  False\n",
      "So True\n",
      "we True\n",
      "would True\n",
      "always True\n",
      "bring True\n",
      "an True\n",
      "xtra True\n",
      "t True\n",
      "shirt True\n",
      "and True\n",
      "pair True\n",
      "of True\n",
      "jeans True\n",
      "  False\n",
      "This True\n",
      "went True\n",
      "on True\n",
      "for True\n",
      "years True\n",
      "  False\n",
      "Looking True\n",
      "back True\n",
      "now True\n",
      "on True\n",
      "how True\n",
      "naive True\n",
      "we True\n",
      "were True\n",
      "  False\n",
      "it True\n",
      "s True\n",
      "just True\n",
      "hilarious True\n",
      "  False\n",
      "I True\n",
      "was True\n",
      "never True\n",
      "able True\n",
      "to True\n",
      "walk True\n",
      "out True\n",
      "of True\n",
      "that True\n",
      "bar True\n",
      "    False\n",
      "had True\n",
      "to True\n",
      "crawl True\n",
      "out True\n",
      "  False\n",
      "So True\n",
      "much True\n",
      "booze True\n",
      "  False\n",
      "Cheers True\n",
      "  False\n",
      "Ride True\n",
      "safe True\n",
      "  False\n",
      "boys True\n"
     ]
    }
   ],
   "source": [
    "# identifying alphabetical characters\n",
    "for token in doc:\n",
    "    print(token,token.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7561177d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny False\n",
      "story False\n",
      "  False\n",
      "I True\n",
      "went False\n",
      "to True\n",
      "college False\n",
      "in True\n",
      "Las False\n",
      "Vegas False\n",
      "  False\n",
      "This True\n",
      "was True\n",
      "before True\n",
      "I True\n",
      "knew False\n",
      "anything True\n",
      "about True\n",
      "motorcycling False\n",
      "whatsoever False\n",
      "  False\n",
      "Me True\n",
      "and True\n",
      "some True\n",
      "college False\n",
      "buddies False\n",
      "would True\n",
      "always True\n",
      "go True\n",
      "out True\n",
      "on True\n",
      "the True\n",
      "strip False\n",
      "to True\n",
      "the True\n",
      "dance False\n",
      "clubs False\n",
      "  False\n",
      "We True\n",
      "always True\n",
      "ended False\n",
      "up True\n",
      "at True\n",
      "a True\n",
      "bar False\n",
      "called False\n",
      "Hogs False\n",
      "   False\n",
      "Heifers False\n",
      "  False\n",
      "It True\n",
      "s False\n",
      "worth False\n",
      "noting False\n",
      "the True\n",
      "females False\n",
      "working False\n",
      "there True\n",
      "can True\n",
      "outdrink False\n",
      "ANYONE True\n",
      "  False\n",
      "Anyway True\n",
      "  False\n",
      "there True\n",
      "was True\n",
      "a True\n",
      "sign False\n",
      "on True\n",
      "the True\n",
      "front True\n",
      "door False\n",
      "that True\n",
      "read False\n",
      "  False\n",
      "No True\n",
      "Club False\n",
      "Colors False\n",
      "   False\n",
      "So True\n",
      "we True\n",
      "lose False\n",
      "our True\n",
      "ties False\n",
      "and True\n",
      "blazers False\n",
      "before True\n",
      "heading False\n",
      "there True\n",
      "  False\n",
      "Also True\n",
      "we True\n",
      "assumed False\n",
      "bright False\n",
      "colors False\n",
      "like False\n",
      "red False\n",
      "  False\n",
      "yellow False\n",
      "  False\n",
      "green False\n",
      "etc False\n",
      "were True\n",
      "not True\n",
      "allowed False\n",
      "  False\n",
      "So True\n",
      "we True\n",
      "would True\n",
      "always True\n",
      "bring False\n",
      "an True\n",
      "xtra False\n",
      "t False\n",
      "shirt False\n",
      "and True\n",
      "pair False\n",
      "of True\n",
      "jeans False\n",
      "  False\n",
      "This True\n",
      "went False\n",
      "on True\n",
      "for True\n",
      "years False\n",
      "  False\n",
      "Looking False\n",
      "back True\n",
      "now True\n",
      "on True\n",
      "how True\n",
      "naive False\n",
      "we True\n",
      "were True\n",
      "  False\n",
      "it True\n",
      "s False\n",
      "just True\n",
      "hilarious False\n",
      "  False\n",
      "I True\n",
      "was True\n",
      "never True\n",
      "able False\n",
      "to True\n",
      "walk False\n",
      "out True\n",
      "of True\n",
      "that True\n",
      "bar False\n",
      "    False\n",
      "had True\n",
      "to True\n",
      "crawl False\n",
      "out True\n",
      "  False\n",
      "So True\n",
      "much True\n",
      "booze False\n",
      "  False\n",
      "Cheers False\n",
      "  False\n",
      "Ride False\n",
      "safe False\n",
      "  False\n",
      "boys False\n"
     ]
    }
   ],
   "source": [
    "# identifying stop words in a document\n",
    "for token in doc:\n",
    "    print(token,token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc47ec-0b3d-4b9c-b1f0-6182f4373455",
   "metadata": {},
   "source": [
    "## Tag-of-speech\n",
    "\n",
    "- **part-of-speech** are the grammatical units of language: verbs, nouns, adjectives, adverbs, pronouns, prepositions\n",
    "\n",
    "- part-of-speech can be used to explore syntax\n",
    "\n",
    "- - Each token in a spaCy doc has two part-of-speech attributes:\n",
    "    - pos_\n",
    "    - tag_\n",
    "- tag_ can be language specific \n",
    "- pos_ contains the simplified tag of the universal part-of-speech tagset\n",
    " \n",
    "- pos_ can be used as an alternative to stop words\n",
    "\n",
    "- pos_ can be classified into two categories \n",
    "\n",
    "- pronouns, prepositions, conjunctions, determiners: \n",
    "    - called **function words**\n",
    "    - their main function is to create grammatical relationships in a sentence\n",
    "    - not very informative\n",
    "\n",
    "- nouns, verbs, adjectives and adverbs: \n",
    "    - **content** words\n",
    "    - the meaning of a sentence depends on them\n",
    "    \n",
    "\n",
    "- We can use **part-of-speech tags** to select the word types\n",
    "\n",
    "https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/13-POS-Keywords.html#\n",
    "\n",
    "- Part-of-speech tags can be used to make a selection among tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264204b-cbd6-4b92-8c34-fb6e9dc2590f",
   "metadata": {},
   "source": [
    "spaCy has been trained to recognize pos_ according to the context in which the word appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92654237-c8e2-48ff-a1f7-2e9f18eab4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You PRON\n",
      "need VERB\n",
      "to PART\n",
      "write VERB\n",
      "an DET\n",
      "abstract NOUN\n"
     ]
    }
   ],
   "source": [
    "sentence1 = 'You need to write an abstract'\n",
    "token_sentence1 = nlp(sentence1)\n",
    "for token in token_sentence1:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "949147e6-59b4-4f6a-ac2d-80cdfba88b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At ADP\n",
      "his PRON\n",
      "age NOUN\n",
      ", PUNCT\n",
      "he PRON\n",
      "still ADV\n",
      "fails VERB\n",
      "to PART\n",
      "abstract ADJ\n",
      "certain ADJ\n",
      "concepts NOUN\n"
     ]
    }
   ],
   "source": [
    "sentence2 = 'At his age, he still fails to abstract certain concepts'\n",
    "token_sentence2 = nlp(sentence2)\n",
    "for token in token_sentence2:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ead88217-cc57-425c-ad24-f04b82a5954d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He PRON\n",
      "ages VERB\n",
      "well ADV\n"
     ]
    }
   ],
   "source": [
    "sentence3 = \"He ages well\"\n",
    "token_sentence3 = nlp(sentence3)\n",
    "for token in token_sentence3:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f70c13-ecfd-449a-bf92-99afcc87f91a",
   "metadata": {},
   "source": [
    "### Tokens and pos_ of doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5eceb461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny ADJ adjective\n",
      "story NOUN noun\n",
      "  SPACE space\n",
      "I PRON pronoun\n",
      "went VERB verb\n",
      "to ADP adposition\n",
      "college NOUN noun\n",
      "in ADP adposition\n",
      "Las PROPN proper noun\n",
      "Vegas PROPN proper noun\n",
      "  SPACE space\n",
      "This PRON pronoun\n",
      "was AUX auxiliary\n",
      "before SCONJ subordinating conjunction\n",
      "I PRON pronoun\n",
      "knew VERB verb\n",
      "anything PRON pronoun\n",
      "about ADP adposition\n",
      "motorcycling VERB verb\n",
      "whatsoever ADP adposition\n",
      "  SPACE space\n",
      "Me PRON pronoun\n",
      "and CCONJ coordinating conjunction\n",
      "some DET determiner\n",
      "college NOUN noun\n",
      "buddies NOUN noun\n",
      "would AUX auxiliary\n",
      "always ADV adverb\n",
      "go VERB verb\n",
      "out ADP adposition\n",
      "on ADP adposition\n",
      "the DET determiner\n",
      "strip NOUN noun\n",
      "to ADP adposition\n",
      "the DET determiner\n",
      "dance NOUN noun\n",
      "clubs NOUN noun\n",
      "  SPACE space\n",
      "We PRON pronoun\n",
      "always ADV adverb\n",
      "ended VERB verb\n",
      "up ADP adposition\n",
      "at ADP adposition\n",
      "a DET determiner\n",
      "bar NOUN noun\n",
      "called VERB verb\n",
      "Hogs PROPN proper noun\n",
      "   SPACE space\n",
      "Heifers PROPN proper noun\n",
      "  SPACE space\n",
      "It PRON pronoun\n",
      "s VERB verb\n",
      "worth ADJ adjective\n",
      "noting VERB verb\n",
      "the DET determiner\n",
      "females NOUN noun\n",
      "working VERB verb\n",
      "there ADV adverb\n",
      "can AUX auxiliary\n",
      "outdrink VERB verb\n",
      "ANYONE PRON pronoun\n",
      "  SPACE space\n",
      "Anyway ADV adverb\n",
      "  SPACE space\n",
      "there PRON pronoun\n",
      "was VERB verb\n",
      "a DET determiner\n",
      "sign NOUN noun\n",
      "on ADP adposition\n",
      "the DET determiner\n",
      "front ADJ adjective\n",
      "door NOUN noun\n",
      "that PRON pronoun\n",
      "read VERB verb\n",
      "  SPACE space\n",
      "No DET determiner\n",
      "Club PROPN proper noun\n",
      "Colors PROPN proper noun\n",
      "   SPACE space\n",
      "So ADV adverb\n",
      "we PRON pronoun\n",
      "lose VERB verb\n",
      "our PRON pronoun\n",
      "ties NOUN noun\n",
      "and CCONJ coordinating conjunction\n",
      "blazers NOUN noun\n",
      "before ADP adposition\n",
      "heading VERB verb\n",
      "there ADV adverb\n",
      "  SPACE space\n",
      "Also ADV adverb\n",
      "we PRON pronoun\n",
      "assumed VERB verb\n",
      "bright ADJ adjective\n",
      "colors NOUN noun\n",
      "like ADP adposition\n",
      "red ADJ adjective\n",
      "  SPACE space\n",
      "yellow ADJ adjective\n",
      "  SPACE space\n",
      "green ADJ adjective\n",
      "etc NOUN noun\n",
      "were AUX auxiliary\n",
      "not PART particle\n",
      "allowed VERB verb\n",
      "  SPACE space\n",
      "So ADV adverb\n",
      "we PRON pronoun\n",
      "would AUX auxiliary\n",
      "always ADV adverb\n",
      "bring VERB verb\n",
      "an DET determiner\n",
      "xtra PROPN proper noun\n",
      "t PROPN proper noun\n",
      "shirt NOUN noun\n",
      "and CCONJ coordinating conjunction\n",
      "pair NOUN noun\n",
      "of ADP adposition\n",
      "jeans NOUN noun\n",
      "  SPACE space\n",
      "This PRON pronoun\n",
      "went VERB verb\n",
      "on ADP adposition\n",
      "for ADP adposition\n",
      "years NOUN noun\n",
      "  SPACE space\n",
      "Looking VERB verb\n",
      "back ADV adverb\n",
      "now ADV adverb\n",
      "on ADP adposition\n",
      "how SCONJ subordinating conjunction\n",
      "naive ADJ adjective\n",
      "we PRON pronoun\n",
      "were AUX auxiliary\n",
      "  SPACE space\n",
      "it PRON pronoun\n",
      "s VERB verb\n",
      "just ADV adverb\n",
      "hilarious ADJ adjective\n",
      "  SPACE space\n",
      "I PRON pronoun\n",
      "was AUX auxiliary\n",
      "never ADV adverb\n",
      "able ADJ adjective\n",
      "to PART particle\n",
      "walk VERB verb\n",
      "out ADP adposition\n",
      "of ADP adposition\n",
      "that DET determiner\n",
      "bar NOUN noun\n",
      "    SPACE space\n",
      "had VERB verb\n",
      "to PART particle\n",
      "crawl VERB verb\n",
      "out ADP adposition\n",
      "  SPACE space\n",
      "So ADV adverb\n",
      "much ADJ adjective\n",
      "booze NOUN noun\n",
      "  SPACE space\n",
      "Cheers PROPN proper noun\n",
      "  SPACE space\n",
      "Ride PROPN proper noun\n",
      "safe ADJ adjective\n",
      "  SPACE space\n",
      "boys NOUN noun\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_, spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9299ec-4d8e-4788-a67b-c1da7d7799ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "We want to make the list of the nouns in doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d83edd9f-bd55-439d-9480-2369f2078d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns=[]\n",
    "for token in doc:\n",
    "    if token.pos_== 'NOUN':\n",
    "       nouns.append(token.text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6956211b-0c7e-4a4b-a855-e3e0ab4bfb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['story',\n",
       " 'college',\n",
       " 'college',\n",
       " 'buddies',\n",
       " 'strip',\n",
       " 'dance',\n",
       " 'clubs',\n",
       " 'bar',\n",
       " 'females',\n",
       " 'sign',\n",
       " 'door',\n",
       " 'ties',\n",
       " 'blazers',\n",
       " 'colors',\n",
       " 'etc',\n",
       " 'shirt',\n",
       " 'pair',\n",
       " 'jeans',\n",
       " 'years',\n",
       " 'bar',\n",
       " 'booze',\n",
       " 'boys']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bde4137e-fd91-4526-ba96-c52954f4e6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'college': 2, 'bar': 2, 'story': 1, 'buddies': 1, 'strip': 1, 'dance': 1, 'clubs': 1, 'females': 1, 'sign': 1, 'door': 1, 'ties': 1, 'blazers': 1, 'colors': 1, 'etc': 1, 'shirt': 1, 'pair': 1, 'jeans': 1, 'years': 1, 'booze': 1, 'boys': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "nouns_count = Counter(nouns)\n",
    "print(nouns_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f5273582-1357-4374-ad9a-d93fdf23b19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('college', 2),\n",
       " ('bar', 2),\n",
       " ('story', 1),\n",
       " ('buddies', 1),\n",
       " ('strip', 1),\n",
       " ('dance', 1),\n",
       " ('clubs', 1),\n",
       " ('females', 1),\n",
       " ('sign', 1),\n",
       " ('door', 1),\n",
       " ('ties', 1),\n",
       " ('blazers', 1),\n",
       " ('colors', 1),\n",
       " ('etc', 1),\n",
       " ('shirt', 1),\n",
       " ('pair', 1),\n",
       " ('jeans', 1),\n",
       " ('years', 1),\n",
       " ('booze', 1),\n",
       " ('boys', 1)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_count.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19ae7e-6efa-4c84-828d-d66ec74c0054",
   "metadata": {},
   "source": [
    "### Specific functions of Textacy to extract words according to their pos\n",
    "The output is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "148c0b41-b88f-4720-b041-ac87bc19d7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Funny, story, went, college, Las, Vegas, knew, motorcycling, whatsoever, college, buddies, strip, dance, clubs, ended, bar, called, Hogs, Heifers, s, worth, noting, females, working, outdrink, sign, door, read, Club, Colors, lose, ties, blazers, heading, assumed, bright, colors, like, red, yellow, green, etc, allowed, bring, xtra, t, shirt, pair, jeans, went, years, Looking, naive, s, hilarious, able, walk, bar, crawl, booze, Cheers, Ride, safe, boys]\n"
     ]
    }
   ],
   "source": [
    "token_alt =textacy.extract.words(doc)\n",
    "print(list(token_alt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "59a618cd-41d6-4051-8e08-4d44a709b1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny|story|college|college|buddies|strip|dance|clubs|bar|worth|females|sign|door|ties|blazers|bright|colors|red|yellow|green|etc|shirt|pair|jeans|years|naive|hilarious|able|bar|booze|safe|boys\n"
     ]
    }
   ],
   "source": [
    "# The input file must be a doc \n",
    "tokens1=textacy.extract.words(doc, include_pos={\"ADJ\",\"NOUN\"})\n",
    "print(list(tokens1))\n",
    "#print(*[t for t in tokens1], sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "022f9743-d454-4479-96b7-32b994ec99e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "college|college|bar|bar\n"
     ]
    }
   ],
   "source": [
    "tokens2=textacy.extract.words(doc, include_pos={\"ADJ\",\"NOUN\"},min_freq=2)\n",
    "print(list(tokens2)\n",
    "#print(*[t for t in tokens2], sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47fcb8-cc20-43c6-8a5e-ccdfaab7ef4f",
   "metadata": {},
   "source": [
    "### Tags \n",
    "A more detailled classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98548084-eaa7-4933-a294-760e6214b9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny JJ adjective (English), other noun-modifier (Chinese)\n",
      "story NN noun, singular or mass\n",
      "  _SP whitespace\n",
      "I PRP pronoun, personal\n",
      "went VBD verb, past tense\n",
      "to IN conjunction, subordinating or preposition\n",
      "college NN noun, singular or mass\n",
      "in IN conjunction, subordinating or preposition\n",
      "Las NNP noun, proper singular\n",
      "Vegas NNP noun, proper singular\n",
      "  _SP whitespace\n",
      "This DT determiner\n",
      "was VBD verb, past tense\n",
      "before IN conjunction, subordinating or preposition\n",
      "I PRP pronoun, personal\n",
      "knew VBD verb, past tense\n",
      "anything NN noun, singular or mass\n",
      "about IN conjunction, subordinating or preposition\n",
      "motorcycling VBG verb, gerund or present participle\n",
      "whatsoever IN conjunction, subordinating or preposition\n",
      "  _SP whitespace\n",
      "Me PRP pronoun, personal\n",
      "and CC conjunction, coordinating\n",
      "some DT determiner\n",
      "college NN noun, singular or mass\n",
      "buddies NNS noun, plural\n",
      "would MD verb, modal auxiliary\n",
      "always RB adverb\n",
      "go VB verb, base form\n",
      "out RP adverb, particle\n",
      "on IN conjunction, subordinating or preposition\n",
      "the DT determiner\n",
      "strip NN noun, singular or mass\n",
      "to IN conjunction, subordinating or preposition\n",
      "the DT determiner\n",
      "dance NN noun, singular or mass\n",
      "clubs NNS noun, plural\n",
      "  _SP whitespace\n",
      "We PRP pronoun, personal\n",
      "always RB adverb\n",
      "ended VBD verb, past tense\n",
      "up RP adverb, particle\n",
      "at IN conjunction, subordinating or preposition\n",
      "a DT determiner\n",
      "bar NN noun, singular or mass\n",
      "called VBN verb, past participle\n",
      "Hogs NNP noun, proper singular\n",
      "   _SP whitespace\n",
      "Heifers NNP noun, proper singular\n",
      "  _SP whitespace\n",
      "It PRP pronoun, personal\n",
      "s VBZ verb, 3rd person singular present\n",
      "worth JJ adjective (English), other noun-modifier (Chinese)\n",
      "noting VBG verb, gerund or present participle\n",
      "the DT determiner\n",
      "females NNS noun, plural\n",
      "working VBG verb, gerund or present participle\n",
      "there RB adverb\n",
      "can MD verb, modal auxiliary\n",
      "outdrink VB verb, base form\n",
      "ANYONE NN noun, singular or mass\n",
      "  _SP whitespace\n",
      "Anyway RB adverb\n",
      "  _SP whitespace\n",
      "there EX existential there\n",
      "was VBD verb, past tense\n",
      "a DT determiner\n",
      "sign NN noun, singular or mass\n",
      "on IN conjunction, subordinating or preposition\n",
      "the DT determiner\n",
      "front JJ adjective (English), other noun-modifier (Chinese)\n",
      "door NN noun, singular or mass\n",
      "that WDT wh-determiner\n",
      "read VBD verb, past tense\n",
      "  _SP whitespace\n",
      "No DT determiner\n",
      "Club NNP noun, proper singular\n",
      "Colors NNPS noun, proper plural\n",
      "   _SP whitespace\n",
      "So RB adverb\n",
      "we PRP pronoun, personal\n",
      "lose VBP verb, non-3rd person singular present\n",
      "our PRP$ pronoun, possessive\n",
      "ties NNS noun, plural\n",
      "and CC conjunction, coordinating\n",
      "blazers NNS noun, plural\n",
      "before IN conjunction, subordinating or preposition\n",
      "heading VBG verb, gerund or present participle\n",
      "there RB adverb\n",
      "  _SP whitespace\n",
      "Also RB adverb\n",
      "we PRP pronoun, personal\n",
      "assumed VBD verb, past tense\n",
      "bright JJ adjective (English), other noun-modifier (Chinese)\n",
      "colors NNS noun, plural\n",
      "like IN conjunction, subordinating or preposition\n",
      "red JJ adjective (English), other noun-modifier (Chinese)\n",
      "  _SP whitespace\n",
      "yellow JJ adjective (English), other noun-modifier (Chinese)\n",
      "  _SP whitespace\n",
      "green JJ adjective (English), other noun-modifier (Chinese)\n",
      "etc NN noun, singular or mass\n",
      "were VBD verb, past tense\n",
      "not RB adverb\n",
      "allowed VBN verb, past participle\n",
      "  _SP whitespace\n",
      "So RB adverb\n",
      "we PRP pronoun, personal\n",
      "would MD verb, modal auxiliary\n",
      "always RB adverb\n",
      "bring VB verb, base form\n",
      "an DT determiner\n",
      "xtra NNP noun, proper singular\n",
      "t NNP noun, proper singular\n",
      "shirt NN noun, singular or mass\n",
      "and CC conjunction, coordinating\n",
      "pair NN noun, singular or mass\n",
      "of IN conjunction, subordinating or preposition\n",
      "jeans NNS noun, plural\n",
      "  _SP whitespace\n",
      "This DT determiner\n",
      "went VBD verb, past tense\n",
      "on RP adverb, particle\n",
      "for IN conjunction, subordinating or preposition\n",
      "years NNS noun, plural\n",
      "  _SP whitespace\n",
      "Looking VBG verb, gerund or present participle\n",
      "back RB adverb\n",
      "now RB adverb\n",
      "on IN conjunction, subordinating or preposition\n",
      "how WRB wh-adverb\n",
      "naive JJ adjective (English), other noun-modifier (Chinese)\n",
      "we PRP pronoun, personal\n",
      "were VBD verb, past tense\n",
      "  _SP whitespace\n",
      "it PRP pronoun, personal\n",
      "s VBZ verb, 3rd person singular present\n",
      "just RB adverb\n",
      "hilarious JJ adjective (English), other noun-modifier (Chinese)\n",
      "  _SP whitespace\n",
      "I PRP pronoun, personal\n",
      "was VBD verb, past tense\n",
      "never RB adverb\n",
      "able JJ adjective (English), other noun-modifier (Chinese)\n",
      "to TO infinitival \"to\"\n",
      "walk VB verb, base form\n",
      "out IN conjunction, subordinating or preposition\n",
      "of IN conjunction, subordinating or preposition\n",
      "that DT determiner\n",
      "bar NN noun, singular or mass\n",
      "    _SP whitespace\n",
      "had VBD verb, past tense\n",
      "to TO infinitival \"to\"\n",
      "crawl VB verb, base form\n",
      "out RP adverb, particle\n",
      "  _SP whitespace\n",
      "So RB adverb\n",
      "much JJ adjective (English), other noun-modifier (Chinese)\n",
      "booze NN noun, singular or mass\n",
      "  _SP whitespace\n",
      "Cheers NNPS noun, proper plural\n",
      "  _SP whitespace\n",
      "Ride NNP noun, proper singular\n",
      "safe JJ adjective (English), other noun-modifier (Chinese)\n",
      "  _SP whitespace\n",
      "boys NNS noun, plural\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token,token.tag_,spacy.explain(token.tag_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e985845c-2b34-4073-9b29-a75cd707807b",
   "metadata": {},
   "source": [
    "### dep_ structure of dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c4c38f0b-b6ff-4391-a868-5e2393fc59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "499541fe-635f-46f4-9260-86d5ec837ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"908907fb763b42cc9fe4c55c3ddd12d0-0\" class=\"displacy\" width=\"590\" height=\"227.0\" direction=\"ltr\" style=\"max-width: none; height: 227.0px; color: yellow; background: black; font-family: Gill Sans; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">You</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">need</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">write</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">abstract</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-908907fb763b42cc9fe4c55c3ddd12d0-0-0\" stroke-width=\"2px\" d=\"M62,92.0 62,77.0 137.0,77.0 137.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-908907fb763b42cc9fe4c55c3ddd12d0-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,94.0 L58,86.0 66,86.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-908907fb763b42cc9fe4c55c3ddd12d0-0-1\" stroke-width=\"2px\" d=\"M242,92.0 242,77.0 317.0,77.0 317.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-908907fb763b42cc9fe4c55c3ddd12d0-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M242,94.0 L238,86.0 246,86.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-908907fb763b42cc9fe4c55c3ddd12d0-0-2\" stroke-width=\"2px\" d=\"M152,92.0 152,62.0 320.0,62.0 320.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-908907fb763b42cc9fe4c55c3ddd12d0-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M320.0,94.0 L324.0,86.0 316.0,86.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-908907fb763b42cc9fe4c55c3ddd12d0-0-3\" stroke-width=\"2px\" d=\"M422,92.0 422,77.0 497.0,77.0 497.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-908907fb763b42cc9fe4c55c3ddd12d0-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M422,94.0 L418,86.0 426,86.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-908907fb763b42cc9fe4c55c3ddd12d0-0-4\" stroke-width=\"2px\" d=\"M332,92.0 332,62.0 500.0,62.0 500.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-908907fb763b42cc9fe4c55c3ddd12d0-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M500.0,94.0 L504.0,86.0 496.0,86.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set some display options for the visualizer\n",
    "options = {\"compact\": True, \"distance\": 90, \"color\": \"yellow\", \"bg\": \"black\", \"font\": \"Gill Sans\"}\n",
    "\n",
    "displacy.render(token_sentence1, style=\"dep\", options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d386300-140d-4603-b699-c30880f8f1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny amod adjectival modifier\n",
      "story ROOT root\n",
      "  dep unclassified dependent\n",
      "I nsubj nominal subject\n",
      "went ROOT root\n",
      "to prep prepositional modifier\n",
      "college pobj object of preposition\n",
      "in prep prepositional modifier\n",
      "Las compound compound\n",
      "Vegas pobj object of preposition\n",
      "  dep unclassified dependent\n",
      "This nsubj nominal subject\n",
      "was ROOT root\n",
      "before mark marker\n",
      "I nsubj nominal subject\n",
      "knew advcl adverbial clause modifier\n",
      "anything dobj direct object\n",
      "about prep prepositional modifier\n",
      "motorcycling pcomp complement of preposition\n",
      "whatsoever advmod adverbial modifier\n",
      "  dep unclassified dependent\n",
      "Me pobj object of preposition\n",
      "and cc coordinating conjunction\n",
      "some det determiner\n",
      "college compound compound\n",
      "buddies conj conjunct\n",
      "would aux auxiliary\n",
      "always advmod adverbial modifier\n",
      "go conj conjunct\n",
      "out prt particle\n",
      "on prep prepositional modifier\n",
      "the det determiner\n",
      "strip pobj object of preposition\n",
      "to prep prepositional modifier\n",
      "the det determiner\n",
      "dance compound compound\n",
      "clubs pobj object of preposition\n",
      "  dep unclassified dependent\n",
      "We nsubj nominal subject\n",
      "always advmod adverbial modifier\n",
      "ended relcl relative clause modifier\n",
      "up prt particle\n",
      "at prep prepositional modifier\n",
      "a det determiner\n",
      "bar pobj object of preposition\n",
      "called acl clausal modifier of noun (adjectival clause)\n",
      "Hogs compound compound\n",
      "   dep unclassified dependent\n",
      "Heifers oprd object predicate\n",
      "  dep unclassified dependent\n",
      "It nsubj nominal subject\n",
      "s conj conjunct\n",
      "worth acomp adjectival complement\n",
      "noting xcomp open clausal complement\n",
      "the det determiner\n",
      "females nsubj nominal subject\n",
      "working acl clausal modifier of noun (adjectival clause)\n",
      "there advmod adverbial modifier\n",
      "can aux auxiliary\n",
      "outdrink ccomp clausal complement\n",
      "ANYONE dobj direct object\n",
      "  dep unclassified dependent\n",
      "Anyway intj interjection\n",
      "  dep unclassified dependent\n",
      "there expl expletive\n",
      "was ROOT root\n",
      "a det determiner\n",
      "sign attr attribute\n",
      "on prep prepositional modifier\n",
      "the det determiner\n",
      "front amod adjectival modifier\n",
      "door pobj object of preposition\n",
      "that nsubj nominal subject\n",
      "read relcl relative clause modifier\n",
      "  dep unclassified dependent\n",
      "No det determiner\n",
      "Club compound compound\n",
      "Colors dobj direct object\n",
      "   dep unclassified dependent\n",
      "So advmod adverbial modifier\n",
      "we nsubj nominal subject\n",
      "lose ROOT root\n",
      "our poss possession modifier\n",
      "ties dobj direct object\n",
      "and cc coordinating conjunction\n",
      "blazers conj conjunct\n",
      "before prep prepositional modifier\n",
      "heading pcomp complement of preposition\n",
      "there advmod adverbial modifier\n",
      "  dep unclassified dependent\n",
      "Also advmod adverbial modifier\n",
      "we nsubj nominal subject\n",
      "assumed conj conjunct\n",
      "bright amod adjectival modifier\n",
      "colors nsubjpass nominal subject (passive)\n",
      "like prep prepositional modifier\n",
      "red amod adjectival modifier\n",
      "  dep unclassified dependent\n",
      "yellow amod adjectival modifier\n",
      "  dep unclassified dependent\n",
      "green compound compound\n",
      "etc pobj object of preposition\n",
      "were auxpass auxiliary (passive)\n",
      "not neg negation modifier\n",
      "allowed ccomp clausal complement\n",
      "  dep unclassified dependent\n",
      "So advmod adverbial modifier\n",
      "we nsubj nominal subject\n",
      "would aux auxiliary\n",
      "always advmod adverbial modifier\n",
      "bring ROOT root\n",
      "an det determiner\n",
      "xtra compound compound\n",
      "t compound compound\n",
      "shirt dobj direct object\n",
      "and cc coordinating conjunction\n",
      "pair conj conjunct\n",
      "of prep prepositional modifier\n",
      "jeans pobj object of preposition\n",
      "  dep unclassified dependent\n",
      "This nsubj nominal subject\n",
      "went ccomp clausal complement\n",
      "on prt particle\n",
      "for prep prepositional modifier\n",
      "years pobj object of preposition\n",
      "  dep unclassified dependent\n",
      "Looking advcl adverbial clause modifier\n",
      "back advmod adverbial modifier\n",
      "now advmod adverbial modifier\n",
      "on prep prepositional modifier\n",
      "how advmod adverbial modifier\n",
      "naive acomp adjectival complement\n",
      "we nsubj nominal subject\n",
      "were pcomp complement of preposition\n",
      "  dep unclassified dependent\n",
      "it nsubj nominal subject\n",
      "s conj conjunct\n",
      "just advmod adverbial modifier\n",
      "hilarious acomp adjectival complement\n",
      "  dep unclassified dependent\n",
      "I nsubj nominal subject\n",
      "was ccomp clausal complement\n",
      "never neg negation modifier\n",
      "able acomp adjectival complement\n",
      "to aux auxiliary\n",
      "walk xcomp open clausal complement\n",
      "out prep prepositional modifier\n",
      "of prep prepositional modifier\n",
      "that det determiner\n",
      "bar pobj object of preposition\n",
      "    dep unclassified dependent\n",
      "had conj conjunct\n",
      "to aux auxiliary\n",
      "crawl xcomp open clausal complement\n",
      "out prt particle\n",
      "  dep unclassified dependent\n",
      "So advmod adverbial modifier\n",
      "much amod adjectival modifier\n",
      "booze nmod modifier of nominal\n",
      "  dep unclassified dependent\n",
      "Cheers compound compound\n",
      "  dep unclassified dependent\n",
      "Ride nmod modifier of nominal\n",
      "safe amod adjectival modifier\n",
      "  dep unclassified dependent\n",
      "boys dobj direct object\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token,token.dep_,spacy.explain(token.dep_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d20d72",
   "metadata": {},
   "source": [
    "## Lemmatization/ Stemming\n",
    "\n",
    "- Replacing words with their root: \n",
    "    - \"economic\", \"economics\", \"economically\" all replaced by the stem (the root) \"economy\"\n",
    "    - Porter stemmer (Porter 1980): standard stemming tool for English language text\n",
    "- smaller vocabulary: increase speed of execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9860c850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny funny\n",
      "story story\n",
      "   \n",
      "I I\n",
      "went go\n",
      "to to\n",
      "college college\n",
      "in in\n",
      "Las Las\n",
      "Vegas Vegas\n",
      "   \n",
      "This this\n",
      "was be\n",
      "before before\n",
      "I I\n",
      "knew know\n",
      "anything anything\n",
      "about about\n",
      "motorcycling motorcycle\n",
      "whatsoever whatsoever\n",
      "   \n",
      "Me I\n",
      "and and\n",
      "some some\n",
      "college college\n",
      "buddies buddy\n",
      "would would\n",
      "always always\n",
      "go go\n",
      "out out\n",
      "on on\n",
      "the the\n",
      "strip strip\n",
      "to to\n",
      "the the\n",
      "dance dance\n",
      "clubs club\n",
      "   \n",
      "We we\n",
      "always always\n",
      "ended end\n",
      "up up\n",
      "at at\n",
      "a a\n",
      "bar bar\n",
      "called call\n",
      "Hogs Hogs\n",
      "     \n",
      "Heifers Heifers\n",
      "   \n",
      "It it\n",
      "s s\n",
      "worth worth\n",
      "noting note\n",
      "the the\n",
      "females female\n",
      "working work\n",
      "there there\n",
      "can can\n",
      "outdrink outdrink\n",
      "ANYONE anyone\n",
      "   \n",
      "Anyway anyway\n",
      "   \n",
      "there there\n",
      "was be\n",
      "a a\n",
      "sign sign\n",
      "on on\n",
      "the the\n",
      "front front\n",
      "door door\n",
      "that that\n",
      "read read\n",
      "   \n",
      "No no\n",
      "Club Club\n",
      "Colors Colors\n",
      "     \n",
      "So so\n",
      "we we\n",
      "lose lose\n",
      "our our\n",
      "ties tie\n",
      "and and\n",
      "blazers blazer\n",
      "before before\n",
      "heading head\n",
      "there there\n",
      "   \n",
      "Also also\n",
      "we we\n",
      "assumed assume\n",
      "bright bright\n",
      "colors color\n",
      "like like\n",
      "red red\n",
      "   \n",
      "yellow yellow\n",
      "   \n",
      "green green\n",
      "etc etc\n",
      "were be\n",
      "not not\n",
      "allowed allow\n",
      "   \n",
      "So so\n",
      "we we\n",
      "would would\n",
      "always always\n",
      "bring bring\n",
      "an an\n",
      "xtra xtra\n",
      "t t\n",
      "shirt shirt\n",
      "and and\n",
      "pair pair\n",
      "of of\n",
      "jeans jean\n",
      "   \n",
      "This this\n",
      "went go\n",
      "on on\n",
      "for for\n",
      "years year\n",
      "   \n",
      "Looking look\n",
      "back back\n",
      "now now\n",
      "on on\n",
      "how how\n",
      "naive naive\n",
      "we we\n",
      "were be\n",
      "   \n",
      "it it\n",
      "s s\n",
      "just just\n",
      "hilarious hilarious\n",
      "   \n",
      "I I\n",
      "was be\n",
      "never never\n",
      "able able\n",
      "to to\n",
      "walk walk\n",
      "out out\n",
      "of of\n",
      "that that\n",
      "bar bar\n",
      "       \n",
      "had have\n",
      "to to\n",
      "crawl crawl\n",
      "out out\n",
      "   \n",
      "So so\n",
      "much much\n",
      "booze booze\n",
      "   \n",
      "Cheers Cheers\n",
      "   \n",
      "Ride Ride\n",
      "safe safe\n",
      "   \n",
      "boys boy\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token,token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee8a17-b8d2-4e4b-8e70-d6fd6b981d10",
   "metadata": {},
   "source": [
    "### Analysis of a Doc\n",
    "\n",
    "- Extracting n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "24c99649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Funny story,\n",
       " Las Vegas,\n",
       " motorcycling whatsoever,\n",
       " college buddies,\n",
       " dance clubs,\n",
       " bar called,\n",
       " called Hogs,\n",
       " s worth,\n",
       " worth noting,\n",
       " females working,\n",
       " Club Colors,\n",
       " assumed bright,\n",
       " bright colors,\n",
       " colors like,\n",
       " like red,\n",
       " green etc,\n",
       " xtra t,\n",
       " t shirt,\n",
       " Ride safe]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textacy import extract\n",
    "list(extract.ngrams(doc,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c0f82",
   "metadata": {},
   "source": [
    "### Remark: We can discard some function of the spaCy pipeline\n",
    "\n",
    "We can import selected elements of the pipeline if some component are useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b4021ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_2=spacy.load('en_core_web_sm', disable=[\"parser\",\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "345258cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x24a43f38650>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x24a43f38830>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x24a41957150>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x24a3a8717d0>)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_2.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43245c01",
   "metadata": {},
   "source": [
    "## Working with stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21685c1",
   "metadata": {},
   "source": [
    "- spaCy uses language-specific stop word lists to set the is_stop property for each token\n",
    "- Filtering stop words (and punctuation tokens) is easy\n",
    "- The list of stop words is loaded when a nlp object is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "68b3d1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'without', 'against', 'any', 'anything', 'twelve', 'many', 'about', '’re', 'my', 'various', 'myself', 'these', 'whom', 'nowhere', 'ten', 'neither', 'nevertheless', 'eleven', 'must', 'our', 'same', 'down', 'do', 'often', \"'ll\", 'thru', 'thereby', 'where', 'amongst', 'again', 'being', 'really', 'hereupon', 'here', 'front', 'might', 'should', 'most', 'your', 'yourself', 'sixty', 'formerly', 'serious', 'on', 'only', 'latterly', 'now', 'whenever', 'go', 'empty', 'using', 'i', 'two', 'unless', 'under', 'whence', 'this', 'because', 'he', 'several', 'sometime', 'call', 'someone', 'has', 'see', 'six', 'former', 'somehow', 'n’t', 'please', 'when', 'themselves', 'next', 'she', 'perhaps', 'sometimes', 'did', 'above', 'part', 'always', 'throughout', 'between', 'too', 'say', 'yours', 'first', 'seemed', 'mine', 'whereas', 'ourselves', 'may', 'thus', 'why', 'toward', 'due', 'which', 'its', 'else', 'they', 'whereupon', 'whereby', 'anyhow', 'up', 'nor', 'upon', 'top', 'onto', 'we', 'behind', 'via', 'who', '‘d', 'get', 'whoever', 'whether', 'quite', 'seem', 'herein', 'done', 'during', 'her', '’ve', 'with', 'further', 'seems', 'thence', 'all', \"n't\", 'could', 'there', 'other', 'out', 'ca', 'meanwhile', 'each', 'put', 'but', 'forty', 'fifteen', 'somewhere', 'nobody', 'became', 'becomes', 'can', 'towards', 'three', '’s', 'n‘t', 'used', 'does', 'one', 'already', 're', 'very', 'alone', 'himself', \"'s\", 'seeming', 'move', 'among', 'hundred', 'were', 'while', 'how', 'just', 'everything', 'however', 'moreover', 'everyone', 'afterwards', 'into', 'noone', 'show', 'eight', 'bottom', 'nothing', 'ours', 'no', 'twenty', 'or', 'keep', 'it', 'such', 'if', 'indeed', 'hereby', 'few', 'by', 'still', 'an', 'hers', 'made', 'though', 'both', 'third', 'take', 'much', 'give', 'side', 'rather', 'me', 'name', '‘ve', '’ll', 'of', 'every', 'less', 'around', 'although', 'hereafter', '‘re', 'except', '‘m', 'his', 'well', 'thereupon', 'four', 'below', '‘ll', 'thereafter', 'another', 'off', 'through', 'in', 'back', \"'re\", 'across', 'namely', 'whither', 'something', 'whereafter', 'have', '’d', 'within', 'once', 'so', 'beside', 'becoming', 'whose', \"'m\", 'elsewhere', 'beforehand', 'to', 'is', 'enough', 'then', 'for', 'a', 'at', \"'ve\", 'doing', 'own', 'those', 'full', '’m', '‘s', 'even', 'their', 'cannot', 'than', 'make', 'besides', 'as', 'everywhere', 'after', 'along', 'yourselves', 'been', 'from', 'together', 'before', 'also', 'wherein', 'are', 'would', 'am', 'five', 'until', 'was', 'nine', 'last', 'anyway', 'anywhere', 'him', 'whole', 'some', 'beyond', 'least', 'not', 'whatever', 'the', 'what', 'become', 'over', 'therefore', 'mostly', 'amount', 'itself', 'none', 'yet', 'them', 'and', 'wherever', 'herself', 'that', 'either', 'you', 'therein', 'almost', 'anyone', 'ever', 'latter', 'us', 'will', 'be', 'had', 'per', 'fifty', 'more', 'never', 'since', 'hence', 'otherwise', \"'d\", 'others', 'regarding'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af471fbe",
   "metadata": {},
   "source": [
    "### The list of stop words can be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d67add01",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['down'].is_stop=False\n",
    "nlp.vocab['Dear'].is_stop=True\n",
    "nlp.vocab['Regards'].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2374ceb7",
   "metadata": {},
   "source": [
    "### Extracting Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7522b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lemmas(doc,**kwargs):\n",
    "    return[t.lemma_ for t in textacy.extract.words(doc,**kwargs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fc080aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go|college|college|bar|s|Colors|color|go|s|bar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc = extract_lemmas(doc,min_freq=2)\n",
    "print(*tokenized_doc, sep = \"|\")\n",
    "len(tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3dcf1171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funny|story|college|college|buddy|strip|dance|club|bar|worth|female|sign|door|tie|blazer|bright|color|red|yellow|green|etc|shirt|pair|jean|year|naive|hilarious|able|bar|booze|safe|boy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc = extract_lemmas(doc,  include_pos={\"ADJ\",\"NOUN\"})\n",
    "print(*tokenized_doc, sep = \"|\")\n",
    "len(tokenized_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de1950",
   "metadata": {},
   "source": [
    "### Extracting Named entities\n",
    "\n",
    "- The process of detecting entities such as people, locations, organization in texts\n",
    "- In the **Named-entity recognizer** attributes of Doc:\n",
    "    - Doc.ents\n",
    "    - Token.ent_iob_\n",
    "    - Token.ent_type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c7db79eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling whatsoever. Me and some college buddies would always go out on the strip to the dance clubs. We always ended up at a bar called Hogs &amp; Heifers. It's worth noting the females working there can outdrink ANYONE. Anyway, there was a sign on the front door that read 'No Club Colors'. So we lose our ties and blazers before heading there. Also we assumed bright colors like red, yellow, green etc were not allowed. So we would always bring an xtra t-shirt and pair of jeans. This went on for years! Looking back now on how naive we were, it's just hilarious. I was never able to walk out of that bar....had to crawl out! So much booze. <lb><lb>Cheers. Ride safe, boys! \n"
     ]
    }
   ],
   "source": [
    "text0=df.loc[df.index[0],'text'] # selection of text by using df.index[list]\n",
    "print(text0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c1df76c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funny story  I went to college in Las Vegas  This was before I knew anything about motorcycling whatsoever  Me and some college buddies would always go out on the strip to the dance clubs  We always ended up at a bar called Hogs   Heifers  It s worth noting the females working there can outdrink ANYONE  Anyway  there was a sign on the front door that read  No Club Colors   So we lose our ties and blazers before heading there  Also we assumed bright colors like red  yellow  green etc were not allowed  So we would always bring an xtra t shirt and pair of jeans  This went on for years  Looking back now on how naive we were  it s just hilarious  I was never able to walk out of that bar    had to crawl out  So much booze  Cheers  Ride safe  boys \n"
     ]
    }
   ],
   "source": [
    "# Preprocesssing with textacy pipeline\n",
    "clean_text0=preproc(text0)\n",
    "\n",
    "print(clean_text0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "509bcb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(166 tokens: \"Funny story  I went to college in Las Vegas  Th...\")'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc0 = textacy.make_spacy_doc(clean_text0,lang=\"en_core_web_sm\")\n",
    "doc0._.preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "84d3a246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Funny story  I went to college in Las Vegas  This was before I knew anything about motorcycling whatsoever  Me and some college buddies would always go out on the strip to the dance clubs  We always ended up at a bar called Hogs   Heifers  It s worth noting the females working there can outdrink ANYONE  Anyway  there was a sign on the front door that read  No Club Colors   So we lose our ties and blazers before heading there  Also we assumed bright colors like red  yellow  green etc were not allowed  So we would always bring an xtra t shirt and pair of jeans  This went on for years  Looking back now on how naive we were  it s just hilarious  I was never able to walk out of that bar    had to crawl out  So much booze  Cheers  Ride safe  boys "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "669c77a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[years]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(textacy.extract.entities(doc, include_types={\"DATE\",\"PRODUCT\",\"ORG\",\"LOCATION\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "49882074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Las Vegas,GPE)(years,DATE)"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f\"({ent.text},{ent.label_})\",end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e3bdf304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Funny story  I went to college in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Las Vegas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "  This was before I knew anything about motorcycling whatsoever  Me and some college buddies would always go out on the strip to the dance clubs  We always ended up at a bar called Hogs   Heifers  It s worth noting the females working there can outdrink ANYONE  Anyway  there was a sign on the front door that read  No Club Colors   So we lose our ties and blazers before heading there  Also we assumed bright colors like red  yellow  green etc were not allowed  So we would always bring an xtra t shirt and pair of jeans  This went on for \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "  Looking back now on how naive we were  it s just hilarious  I was never able to walk out of that bar    had to crawl out  So much booze  Cheers  Ride safe  boys </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc,style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1816c13",
   "metadata": {},
   "source": [
    "# Make a Corpus\n",
    "\n",
    "A textacy.Corpus is an ordered collection of spaCy Doc all processed by the same language pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1798330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "records=df['text']\n",
    "\n",
    "preproc_records=((preproc(text)) for text in records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6d9d49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=textacy.Corpus(\"en_core_web_sm\",data=preproc_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e3b40d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 88521, 2970183)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.n_docs, corpus.n_sents, corpus.n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "30e71825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(166 tokens: \"Funny story  I went to college in Las Vegas  Th...\")'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]._.preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dd20a10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Funny story  I went to college in Las Vegas  This was before I knew anything about motorcycling whatsoever  Me and some college buddies would always go out on the strip to the dance clubs  We always ended up at a bar called Hogs   Heifers  It s worth noting the females working there can outdrink ANYONE  Anyway  there was a sign on the front door that read  No Club Colors   So we lose our ties and blazers before heading there  Also we assumed bright colors like red  yellow  green etc were not allowed  So we would always bring an xtra t shirt and pair of jeans  This went on for years  Looking back now on how naive we were  it s just hilarious  I was never able to walk out of that bar    had to crawl out  So much booze  Cheers  Ride safe  boys "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0eff75-c266-4260-b320-9497584e4424",
   "metadata": {},
   "source": [
    "### Transforming a corpus into an array \n",
    "\n",
    "**textacy.representations.vectorizers** : Transform a collection of tokenized docs into a **doc-term matrix** of shape (# docs, # unique terms), with various ways to filter or limit included terms and flexible weighting schemes for their values.\n",
    "    \n",
    "    \n",
    "https://textacy.readthedocs.io/en/latest/api_reference/representations.html#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e8b8da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = ((term.lemma_ for term in textacy.extract.words(doc,include_pos={\"ADJ\",\"NOUN\"})) for doc in corpus[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d267ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.representations import Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e6f87-5691-4dd9-9360-896370b937ef",
   "metadata": {},
   "source": [
    "### Specification of the Vectorizer\n",
    "tf_type : specify the type of type frequency\n",
    "    tf_type = linear \n",
    "\n",
    "tf_type = can be linear, sqrt, log, binary\n",
    "\n",
    "idf_type : Type of inverse document frequency (idf) to use for weights’ global \n",
    "        can be standard, smooth,bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "02cc71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_alt = Vectorizer( tf_type=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "89da857d-4293-4961-b2a2-6d8a159678c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_alt.weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "eac142f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int32'\n",
       "\twith 505 stored elements and shape (20, 383)>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix_alt = vectorizer_alt.fit_transform(tokenized_docs)\n",
    "doc_term_matrix_alt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56012729-5a01-45ae-aede-8b54f4b34db6",
   "metadata": {},
   "source": [
    "Terms associated with columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ada786f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100k',\n",
       " '35',\n",
       " '4matic',\n",
       " 'EDIT',\n",
       " 'a4',\n",
       " 'able',\n",
       " 'advice',\n",
       " 'altitude',\n",
       " 'anchor',\n",
       " 'answer']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_alt.terms_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4b26b34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "print(doc_term_matrix_alt[:20, vectorizer_alt.vocabulary_terms[\"story\"]].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c5b916cc-2605-4baf-b335-32ffe00e490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs_n = ((term.lemma_ for term in textacy.extract.words(doc,include_pos={\"ADJ\",\"NOUN\"})) for doc in corpus[21:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "efa1554b-d33b-4485-aad3-2d4017385a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int32'\n",
       "\twith 228 stored elements and shape (19, 383)>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_matrix_terms_alt_n = vectorizer_alt.transform(tokenized_docs_n)\n",
    "doc_matrix_terms_alt_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfda4d0-8f08-4df0-a314-86a2175d99bc",
   "metadata": {},
   "source": [
    "## Another example of tokenization and vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2a21c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = ((term.lemma_ for term in textacy.extract.words(doc,include_pos={\"VERB\"})) for doc in corpus[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4ba15709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = Vectorizer( tf_type=\"linear\")\n",
    "vectorizer = Vectorizer(tf_type=\"linear\", idf_type=\"standard\",min_df=5, max_df=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "22b1ab80-d4ff-46d9-8a69-dbda3b1c7dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf * log(n_docs / df) + 1'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7a48a092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 42 stored elements and shape (20, 6)>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix = vectorizer.fit_transform(tokenized_docs)\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a32e6f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.38629436]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.38629436]\n",
      " [2.38629436]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.38629436]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.38629436]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(doc_term_matrix[:20, vectorizer.vocabulary_terms[\"know\"]].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4f3cde6a-5dce-46d4-9e83-83502c56fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = ((term.lemma_ for term in textacy.extract.words(doc,include_pos={\"VERB\"})) for doc in corpus[21:41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e9723958-9b7d-4e19-933c-f6f1ac8790f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 34 stored elements and shape (20, 6)>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_matrix_terms= vectorizer.transform(tokenized_docs)\n",
    "doc_matrix_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "20a69aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.        ]\n",
      " [2.38629436]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.38629436]\n",
      " [2.38629436]\n",
      " [0.        ]\n",
      " [2.38629436]\n",
      " [2.38629436]\n",
      " [0.        ]\n",
      " [2.38629436]]\n"
     ]
    }
   ],
   "source": [
    "print(doc_matrix_terms[:20, vectorizer.vocabulary_terms[\"know\"]].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe90833b-83b2-4ddb-bfe4-e430120dd30a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
